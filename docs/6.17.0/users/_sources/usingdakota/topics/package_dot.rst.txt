.. _topic-package_dot:

Package: DOT
============

The DOT library :cite:p:`Van95` contains nonlinear programming optimizers,
specifically the Broyden-Fletcher-Goldfarb-Shanno (
:ref:`method-dot_bfgs<method-dot_bfgs>`) and Fletcher-Reeves conjugate gradient (
:ref:`method-dot_frcg<method-dot_frcg>`) methods for unconstrained optimization, and the
modified method of feasible directions ( :ref:`method-dot_mmfd<method-dot_mmfd>`),
sequential linear programming ( :ref:`method-dot_slp<method-dot_slp>`), and sequential
quadratic programming ( :ref:`method-dot_sqp<method-dot_sqp>`) methods for constrained
optimization.  To use DOT, one of these methods must be specified.

Specialized handling of linear constraints is supported with DOT;
linear constraint coefficients, bounds, and targets can be provided to
DOT at start-up and tracked internally.

*Method Independent Controls - Stopping Critiera*

Stopping critiera are set by: ``max_iterations``,
``max_function_evaluations``, ``convergence_tolerance``, and
``constraint_tolerance``

Note: The ``convergence_tolerance`` criterion must be satisfied
    for two consecutive iterations before DOT will terminate.

*Method Independent Controls - Output*

The output verbosity specification controls the amount of information
generated by DOT: the ``silent`` and ``quiet`` settings result in header
information, final results, and objective function, constraint, and
parameter information on each iteration; whereas the ``verbose`` and
``debug`` settings add additional information on gradients, search
direction, one-dimensional search results, and parameter scaling
factors.

*Concurrency*

DOT contains no parallel algorithms which can directly take advantage
of concurrent evaluations. However, if ``numerical_gradients`` with
``method_source`` ``dakota`` is specified, then the finite difference
function evaluations can be performed concurrently (using any of the
parallel modes described in the Users Manual).  In
addition, if ``speculative`` is specified, then gradients ( ``dakota``
``numerical`` or ``analytic`` gradients) will be computed on each line
search evaluation in order to balance the load and lower the total run
time in parallel optimization studies.