<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Nonlinear Least Squares &mdash; dakota  documentation</title>
      <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/dakota_theme.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/sandiaheaderlite.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../../_static/jquery.js?v=5d32c60e"></script>
        <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../../_static/documentation_options.js?v=5929fcd5"></script>
        <script src="../../_static/doctools.js?v=9a2dae69"></script>
        <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Topics" href="../topics.html" />
    <link rel="prev" title="Optimization" href="optimization.html" /> 
  
  <meta name="sandia.approval_type" content="formal"/>
  <meta property="sandia.approved" content="SAND2023-13392 O"/>
  <meta name="description" content="The Dakota project delivers both state-of-the-art research and robust, usable software for optimization and UQ."/>
  <meta name="keywords" content="Dakota, optimization, UQ, uncertainty quantification, parametric analysis, design exploration, model calibration, risk analysis"/>
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> dakota
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../setupdakota.html">Getting Started</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../usingdakota.html">Using Dakota</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../introduction/aboutdakota.html">About Dakota</a></li>
<li class="toctree-l2"><a class="reference internal" href="../introduction/helloworld.html">Dakota Beginner’s Tutorial</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples.html">Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../introduction/couplingtosimulations.html">Coupling Dakota to a Simulation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../inputfile.html">Dakota Input File</a></li>
<li class="toctree-l2"><a class="reference internal" href="../running.html">Running Dakota</a></li>
<li class="toctree-l2"><a class="reference internal" href="../output.html">Dakota Output</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../studytypes.html">Study Types</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="parameterstudies.html">Parameter Studies</a></li>
<li class="toctree-l3"><a class="reference internal" href="designofexperiments.html">Design of Experiments</a></li>
<li class="toctree-l3"><a class="reference internal" href="uq.html">Uncertainty Quantification</a></li>
<li class="toctree-l3"><a class="reference internal" href="optimization.html">Optimization</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Nonlinear Least Squares</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="#nonlinear-least-squares-fomulations">Nonlinear Least Squares Fomulations</a></li>
<li class="toctree-l4"><a class="reference internal" href="#nonlinear-least-squares-with-dakota">Nonlinear Least Squares with Dakota</a></li>
<li class="toctree-l4"><a class="reference internal" href="#solution-techniques">Solution Techniques</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#gauss-newton">Gauss-Newton</a></li>
<li class="toctree-l5"><a class="reference internal" href="#nlssol">NLSSOL</a></li>
<li class="toctree-l5"><a class="reference internal" href="#nl2sol">NL2SOL</a></li>
<li class="toctree-l5"><a class="reference internal" href="#additional-features">Additional Features</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="#usage-guidelines">Usage Guidelines</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../topics.html">Topics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../advanced.html">Advanced Topics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../theory.html">Dakota Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reference.html">Keyword Reference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../usingdakotagui/usingdakotagui.html">Using Dakota GUI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../externaltools/externaltools.html">Using External Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../compiling/compiling.html">Compiling Dakota</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../developingdakota/developingdakota.html">Developing Dakota</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../misc/misc.html">Miscellaneous</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">dakota</a>
      </nav>

	  <!-- SNL Lite header -->
	  <div class="snlheader-subsite--wrapper-default">
		<snlheader class="snlheader-subsite" role="snlbanner">
		  <div class="wrapper">
			<a href="https://www.sandia.gov/index.html">
			  <div class="logo-transparent"><p class="logo">Sandia National Laboratories</p></div>
			</a>
			<div class="nav-top">
			  <a class="visuallyhidden" name="mainnav"></a>
			  <div aria-label="main navigation" class="core-nav-transparent core-nav-transparent--visible" role="navigation">
				<ul role="navigation" class="secondary-links">
				  <li id="search-text-link">
					<a aria-label="Search" href="https://www.sandia.gov/search/">Search Sandia.gov</a>
				  </li>
				  <li id="directory-text-link">
					<a href="https://www.sandia.gov/directory.html" aria-expanded="false" aria-label="Site Directory">All Sandia Websites</a>
				  </li>
				</ul>
			  </div>
			</div>
		  </div> 
		</snlheader>
	  </div>	  

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../usingdakota.html">Using Dakota</a></li>
          <li class="breadcrumb-item"><a href="../studytypes.html">Study Types</a></li>
      <li class="breadcrumb-item active">Nonlinear Least Squares</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/usingdakota/studytypes/nonlinearleastsquares.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="nonlinear-least-squares">
<span id="nls"></span><h1>Nonlinear Least Squares<a class="headerlink" href="#nonlinear-least-squares" title="Link to this heading"></a></h1>
<section id="overview">
<span id="nls-overview"></span><h2>Overview<a class="headerlink" href="#overview" title="Link to this heading"></a></h2>
<p>Any Dakota optimization algorithm can be applied to calibration problems
arising in parameter estimation, system identification, and
test/analysis reconciliation. However, nonlinear least-squares methods
are optimization algorithms that exploit the special structure of a sum
of the squares objective function <span id="id1">[<a class="reference internal" href="../../misc/bibliography.html#id111" title="P. E. Gill, W. Murray, and M. H. Wright. Practical Optimization. Academic Press, San Diego, CA, 1981.">GMW81</a>]</span>.</p>
<p>To exploit the problem structure, more granularity is needed in the
response data than is required for a typical optimization problem. That
is, rather than using the sum-of-squares objective function and its
gradient, least-squares iterators require each term used in the
sum-of-squares formulation along with its gradient. This means that the
<span class="math notranslate nohighlight">\(m\)</span> functions in the Dakota response data set consist of the
individual least-squares terms along with any nonlinear inequality and
equality constraints. These individual terms are often called
<em>residuals</em> when they denote differences of observed quantities from
values computed by the model whose parameters are being estimated.</p>
<p>The enhanced granularity needed for nonlinear least-squares algorithms
allows for simplified computation of an approximate Hessian matrix. In
Gauss-Newton-based methods for example, the true Hessian matrix is
approximated by neglecting terms in which residuals multiply Hessians
(matrices of second partial derivatives) of residuals, under the
assumption that the residuals tend towards zero at the solution. As a
result, residual function value and gradient information (first-order
information) is sufficient to define the value, gradient, and
approximate Hessian of the sum-of-squares objective function
(second-order information). See <a class="reference internal" href="#nls-formulations"><span class="std std-ref">formulations</span></a> for
additional details on this approximation.</p>
<p>In practice, least-squares solvers will tend to be significantly more
efficient than general-purpose optimization algorithms when the Hessian
approximation is a good one, e.g., when the residuals tend towards zero
at the solution. Specifically, they can exhibit the quadratic
convergence rates of full Newton methods, even though only first-order
information is used. Gauss-Newton-based least-squares solvers may
experience difficulty when the residuals at the solution are
significant. Dakota has three solvers customized to take advantage of
the sum of squared residuals structure in this problem formulation.
Least squares solvers may experience difficulty when the residuals at
the solution are significant, although experience has shown that
Dakota’s NL2SOL method can handle some problems that are highly
nonlinear and have nonzero residuals at the solution.</p>
</section>
<section id="nonlinear-least-squares-fomulations">
<span id="nls-formulations"></span><h2>Nonlinear Least Squares Fomulations<a class="headerlink" href="#nonlinear-least-squares-fomulations" title="Link to this heading"></a></h2>
<p>Specialized least squares solution algorithms can exploit the structure
of a sum of the squares objective function for problems of the form:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
  \hbox{minimize:} &amp; &amp; f(\mathbf{x}) =
  \sum_{i=1}^{n}[T_i(\mathbf{x})]^2\nonumber\\
  &amp; &amp; \mathbf{x} \in \Re^{p}\nonumber\\
  \hbox{subject to:} &amp; &amp;
  \mathbf{g}_L \leq \mathbf{g(x)} \leq \mathbf{g}_U\nonumber\\
  &amp; &amp; \mathbf{h(x)}=\mathbf{h}_{t}\label{nls:equation02}\\
  &amp; &amp; \mathbf{a}_L \leq \mathbf{A}_i\mathbf{x} \leq
  \mathbf{a}_U\nonumber\\
  &amp; &amp; \mathbf{A}_e\mathbf{x}=\mathbf{a}_{t}\nonumber\\
  &amp; &amp; \mathbf{x}_L \leq \mathbf{x} \leq \mathbf{x}_U\nonumber\end{aligned}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(f(\mathbf{x})\)</span> is the objective function to be minimized
and <span class="math notranslate nohighlight">\(T_i(\mathbf{x})\)</span> is the i<span class="math notranslate nohighlight">\(^{\mathrm{th}}\)</span> least
squares term. The bound, linear, and nonlinear constraints are the same
as described previously for  <a class="reference internal" href="optimization.html#equation-optimformulation">(35)</a>, Optimization Formulations.
Specialized least squares algorithms are generally based on the
Gauss-Newton approximation. When differentiating <span class="math notranslate nohighlight">\(f(\mathbf{x})\)</span>
twice, terms of <span class="math notranslate nohighlight">\(T_i(\mathbf{x})T''_i(\mathbf{x})\)</span> and
<span class="math notranslate nohighlight">\([T'_i(\mathbf{x})]^{2}\)</span> result. By assuming that the former term
tends toward zero near the solution since <span class="math notranslate nohighlight">\(T_i(\mathbf{x})\)</span> tends
toward zero, then the Hessian matrix of second derivatives of
<span class="math notranslate nohighlight">\(f(\mathbf{x})\)</span> can be approximated using only first derivatives
of <span class="math notranslate nohighlight">\(T_i(\mathbf{x})\)</span>. As a result, Gauss-Newton algorithms exhibit
quadratic convergence rates near the solution for those cases when the
Hessian approximation is accurate, i.e. the residuals tend towards zero
at the solution. Thus, by exploiting the structure of the problem, the
second order convergence characteristics of a full Newton algorithm can
be obtained using only first order information from the least squares
terms.</p>
<p>A common example for <span class="math notranslate nohighlight">\(T_i(\mathbf{x})\)</span> might be the difference
between experimental data and model predictions for a response quantity
at a particular location and/or time step, i.e.:</p>
<div class="math notranslate nohighlight">
\[T_i(\mathbf{x}) = R_i(\mathbf{x})-\bar{R_i}
  \label{nls:equation03}\]</div>
<p>where <span class="math notranslate nohighlight">\(R_i(\mathbf{x})\)</span> is the response quantity predicted by the
model and <span class="math notranslate nohighlight">\(\bar{R_i}\)</span> is the corresponding experimental data.
In this case, <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> would have the meaning of model
parameters which are not precisely known and are being calibrated to
match available data. This class of problem is known by the terms
parameter estimation, system identification, model calibration,
test/analysis reconciliation, etc.</p>
</section>
<section id="nonlinear-least-squares-with-dakota">
<h2>Nonlinear Least Squares with Dakota<a class="headerlink" href="#nonlinear-least-squares-with-dakota" title="Link to this heading"></a></h2>
<p>In order to specify a least-squares problem, the responses section of
the Dakota input should be configured using <code class="docutils literal notranslate"><span class="pre">calibration_terms</span></code> (as
opposed to <code class="docutils literal notranslate"><span class="pre">objective_functions</span></code> as for optimization). The calibration
terms refer to the residuals (differences between the simulation model
and the data). Note that Dakota expects the residuals, not the squared
residuals, and offers options for instead returning the simulation
output to Dakota together with a separate <code class="docutils literal notranslate"><span class="pre">calibration_data</span></code> file,
from which residuals will be calculated. Any linear or nonlinear
constraints are handled in an identical way to that of optimization (see
Section Optimization Formulations  <a class="reference internal" href="optimization.html#equation-optimformulation">(35)</a> ; note that neither
Gauss-Newton nor NLSSOL require any constraint augmentation and NL2SOL
supports neither linear nor nonlinear constraints). Gradients of the
least-squares terms and nonlinear constraints are required and should be
specified using either <code class="docutils literal notranslate"><span class="pre">numerical_gradients</span></code>, <code class="docutils literal notranslate"><span class="pre">analytic_gradients</span></code>,
or <code class="docutils literal notranslate"><span class="pre">mixed_gradients</span></code>. Since explicit second derivatives are not used
by the least-squares methods, the <code class="docutils literal notranslate"><span class="pre">no_hessians</span></code> specification should
be used. Dakota’s scaling options, described in
Section Optimization with User-specified or Automatic Scaling, <a class="reference internal" href="optimization.html#opt-additional-scaling-example-figure01"><span class="std std-numref">Listing 53</span></a> can be
used on least-squares problems, using the <code class="docutils literal notranslate"><span class="pre">calibration_term_scales</span></code>
keyword to scale least-squares residuals, if desired.</p>
</section>
<section id="solution-techniques">
<span id="nls-solution"></span><h2>Solution Techniques<a class="headerlink" href="#solution-techniques" title="Link to this heading"></a></h2>
<p>Nonlinear least-squares problems can be solved using the Gauss-Newton
algorithm, which leverages the full Newton method from OPT++, the NLSSOL
algorithm, which is closely related to NPSOL, or the NL2SOL algorithm,
which uses a secant-based algorithm. Details for each are provided
below.</p>
<section id="gauss-newton">
<span id="nls-solution-gauss"></span><h3>Gauss-Newton<a class="headerlink" href="#gauss-newton" title="Link to this heading"></a></h3>
<p>Dakota’s Gauss-Newton algorithm consists of combining an implementation
of the Gauss-Newton Hessian approximation (see
Section  <a class="reference internal" href="#nls-formulations"><span class="std std-ref">Nonlinear Least Squares Fomulations</span></a>) with full Newton optimization
algorithms from the OPT++ package <span id="id2">[<a class="reference internal" href="../../misc/bibliography.html#id200" title="J. C. Meza, R. A. Oliva, P. D. Hough, and P. J. Williams. OPT++: an object oriented toolkit for nonlinear optimization. ACM Transactions on Mathematical Software, 2007.">MOHW07</a>]</span> (see
Section  <a class="reference internal" href="optimization.html#opt-methods-gradient-constrained"><span class="std std-ref">Methods for Constrained Problems</span></a>).
The exact objective function value, exact objective function gradient,
and the approximate objective function Hessian are defined from the
least squares term values and gradients and are passed to the
full-Newton optimizer from the OPT++ software package. As for all of the
Newton-based optimization algorithms in OPT++, unconstrained,
bound-constrained, and generally-constrained problems are supported.
However, for the generally-constrained case, a derivative order mismatch
exists in that the nonlinear interior point full Newton algorithm will
require second-order information for the nonlinear constraints whereas
the Gauss-Newton approximation only requires first order information for
the least squares terms. License: LGPL.</p>
<p>This approach can be selected using the <code class="docutils literal notranslate"><span class="pre">optpp_g_newton</span></code> method
specification. An example specification follows:</p>
<div class="highlight-dakota notranslate"><div class="highlight"><pre><span></span><span class="k">method</span>,
      optpp_g_newton
        max_iterations = 50
        convergence_tolerance = 1e-4
        output debug
</pre></div>
</div>
<p>Refer to the Dakota Reference Manual <a class="reference internal" href="../reference.html#keyword-reference-area"><span class="std std-ref">Keyword Reference</span></a> for more
detail on the input commands for the Gauss-Newton algorithm.</p>
<p>The Gauss-Newton algorithm is gradient-based and is best suited for
efficient navigation to a local least-squares solution in the vicinity
of the initial point. Global optima in multimodal design spaces may be
missed. Gauss-Newton supports bound, linear, and nonlinear constraints.
For the nonlinearly-constrained case, constraint Hessians (required for
full-Newton nonlinear interior point optimization algorithms) are
approximated using quasi-Newton secant updates. Thus, both the objective
and constraint Hessians are approximated using first-order information.</p>
</section>
<section id="nlssol">
<span id="nls-solution-nlssol"></span><h3>NLSSOL<a class="headerlink" href="#nlssol" title="Link to this heading"></a></h3>
<p>The NLSSOL algorithm is bundled with NPSOL. It uses an SQP-based
approach to solve generally-constrained nonlinear least-squares
problems. It periodically employs the Gauss-Newton Hessian approximation
to accelerate the search. Like the Gauss-Newton algorithm of
Section  <a class="reference internal" href="#nls-solution-gauss"><span class="std std-ref">Gauss-Newton</span></a>), its derivative order is
balanced in that it requires only first-order information for the
least-squares terms and nonlinear constraints. License: commercial; see
NPSOL   <a class="reference internal" href="optimization.html#opt-methods-gradient-constrained"><span class="std std-ref">Methods for Constrained Problems</span></a>.</p>
<p>This approach can be selected using the <code class="docutils literal notranslate"><span class="pre">nlssol_sqp</span></code> method
specification. An example specification follows:</p>
<div class="highlight-dakota notranslate"><div class="highlight"><pre><span></span><span class="k">method</span>,
      nlssol_sqp
        convergence_tolerance = 1e-8
</pre></div>
</div>
<p>Refer to the Dakota Reference Manual  <a class="reference internal" href="../reference.html#keyword-reference-area"><span class="std std-ref">Keyword Reference</span></a> for more
detail on the input commands for NLSSOL.</p>
</section>
<section id="nl2sol">
<span id="nls-solution-nl2sol"></span><h3>NL2SOL<a class="headerlink" href="#nl2sol" title="Link to this heading"></a></h3>
<p>The NL2SOL algorithm <span id="id3">[<a class="reference internal" href="../../misc/bibliography.html#id51" title="J. E. Dennis, D. M. Gay, and R. E. Welsch. ALGORITHM 573: NL2SOL–an adaptive nonlinear least-squares algorithm. ACM Trans. Math. Software, 7:369–383, 1981.">DGW81</a>]</span> is a secant-based
least-squares algorithm that is <span class="math notranslate nohighlight">\(q\)</span>-superlinearly convergent. It
adaptively chooses between the Gauss-Newton Hessian approximation and
this approximation augmented by a correction term from a secant update.
NL2SOL tends to be more robust (than conventional Gauss-Newton
approaches) for nonlinear functions and “large residual” problems, i.e.,
least-squares problems for which the residuals do not tend towards zero
at the solution. License: publicly available.</p>
</section>
<section id="additional-features">
<span id="nls-solution-future"></span><h3>Additional Features<a class="headerlink" href="#additional-features" title="Link to this heading"></a></h3>
<p>Dakota’s tailored derivative-based least squares solvers (but not
general optimization solvers) output confidence intervals on estimated
parameters. The reported confidence intervals are univariate
(per-parameter), based on local linearization, and will contain the
true value of the parameters with 95% confidence. Their calculation
essentially follows the exposition in <span id="id4">[<a class="reference internal" href="../../misc/bibliography.html#id261" title="G. A. F. Seber and C. J. Wild. Nonlinear Regression. Wiley, New Jersey, 2003.">SW03</a>]</span> and is
summarized here.</p>
<p>Denote the variance estimate at the optimal calibrated parameters
<span class="math notranslate nohighlight">\(\hat{x}\)</span> by</p>
<div class="math notranslate nohighlight">
\[\hat{\sigma}^2 = \frac{1}{N_{dof}}\sum_{i=1}^{n} T_i(\hat{x})^2,\]</div>
<p>where <span class="math notranslate nohighlight">\(T_i\)</span> are the least squares terms (typically residuals)
discussed above and <span class="math notranslate nohighlight">\(N_{dof} = n - p\)</span> denotes the number of degrees of
freedom (total residuals <span class="math notranslate nohighlight">\(n\)</span> less the number of calibrated parameters
<span class="math notranslate nohighlight">\(p\)</span>). Let</p>
<div class="math notranslate nohighlight">
\[J = \left[ \frac{\partial T(\hat{x})}{\partial x} \right]\]</div>
<p>denote the <span class="math notranslate nohighlight">\(n \times p\)</span> matrix of partial derivatives of the residuals
with respect to the calibrated paramters. Then the standard error <span class="math notranslate nohighlight">\(SE_i\)</span>
for calibrated parameter <span class="math notranslate nohighlight">\(x_i\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[SE_i = \hat{\sigma} \sqrt{\left( J^T J \right)^{-1}_{ii} }.\]</div>
<p>Using a Student’s t-distribution with <span class="math notranslate nohighlight">\(N_{dof}\)</span> degrees of freedom,
the 95% confidence interval for each parameter is given by</p>
<div class="math notranslate nohighlight">
\[\hat{x}_i \pm t(0.975, N_{dof}) \cdot SE_i.\]</div>
<p>In the case where estimated gradients are extremely inaccurate or the
model is very nonlinear, the confidence intervals reported are likely
inaccurate as well.  Further, confidence intervals cannot be
calculated when the number of least-squares terms is less than the
number of parameters to be estimated, when using vendor numerical
gradients, or where there are replicate experiments.  See
<span id="id5">[<a class="reference internal" href="../../misc/bibliography.html#id294" title="K. W. Vugrin, L. P. Swiler, R. M. Roberts, N. J. Stuckey-Mack, and S. P Sullivan. Confidence region estimation techniques for nonlinear regression in groundwater flow: three case studies. Water Resources Research, 2007.">VSR+07</a>]</span> for more details about confidence intervals, and note
that there are alternative approaches such as Bonferroni confidence
intervals and joint confidence intervals based on linear
approximations or F-tests.</p>
<p>Least squares calibration terms (responses) can be weighted. When
observation error variance is provided alongside calibration data, its
inverse is applied to yield the typical variance-weighted least squares
formulation. Alternately, the <code class="docutils literal notranslate"><span class="pre">calibration_terms</span> <span class="pre">weights</span></code>
specification can be used to weight the squared residuals. (Neither set
of weights are adjusted during calibration as they would be in
iteratively re-weighted least squares.) When response scaling is active,
it is applied after error variance weighting and before <code class="docutils literal notranslate"><span class="pre">weights</span></code>
application. The <code class="docutils literal notranslate"><span class="pre">calibration_terms</span></code> keyword documentation in the
Dakota Reference Manual  <a class="reference internal" href="../reference.html#keyword-reference-area"><span class="std std-ref">Keyword Reference</span></a> has more detail about
weighting and scaling of the residual terms.</p>
</section>
</section>
<section id="examples">
<span id="nls-examples"></span><h2>Examples<a class="headerlink" href="#examples" title="Link to this heading"></a></h2>
<p>Both the Rosenbrock and textbook example problems can be formulated as
nonlinear least-squares problems. Refer to <a class="reference internal" href="../examples/additionalexamples.html#additional"><span class="std std-ref">Additional Examples</span></a> for more information on these
formulations.</p>
<p>Figure  <a class="reference internal" href="#nls-nl2sol-example1-figure01"><span class="std std-numref">Listing 54</span></a> shows an excerpt from the
output obtained when running NL2SOL on a five-dimensional problem. Note
that the optimal parameter estimates are printed, followed by the
residual norm and values of the individual residual terms, followed by
the confidence intervals on the parameters.</p>
<div class="literal-block-wrapper docutils container" id="nls-nl2sol-example1-figure01">
<div class="code-block-caption"><span class="caption-number">Listing 54 </span><span class="caption-text">Dakota results for the multiobjective optimization example.</span><a class="headerlink" href="#nls-nl2sol-example1-figure01" title="Link to this code"></a></div>
<div class="highlight-dakota notranslate"><div class="highlight"><pre><span></span>   ------------------------------

   &lt;&lt;&lt;&lt;&lt; Iterator nl2sol completed.
   &lt;&lt;&lt;&lt;&lt; Function evaluation summary: 27 total (26 new, 1 duplicate)
   &lt;&lt;&lt;&lt;&lt; Best parameters          =
                      3.7541004764e-01 x1
                      1.9358463401e+00 x2
                     -1.4646865611e+00 x3
                      1.2867533504e-02 x4
                      2.2122702030e-02 x5
   &lt;&lt;&lt;&lt;&lt; Best residual norm =  7.3924926090e-03; 0.5 * norm^2 =  2.7324473487e-05
   &lt;&lt;&lt;&lt;&lt; Best residual terms      =
                     -2.5698266189e-03
                      4.4759880011e-03
                      9.9223430643e-04
                     -1.0634409194e-03

   ...

   Confidence Interval for x1 is [  3.7116510206e-01,  3.7965499323e-01 ]
   Confidence Interval for x2 is [  1.4845485507e+00,  2.3871441295e+00 ]
   Confidence Interval for x3 is [ -1.9189348458e+00, -1.0104382765e+00 ]
   Confidence Interval for x4 is [  1.1948590669e-02,  1.3786476338e-02 ]
   Confidence Interval for x5 is [  2.0289951664e-02,  2.3955452397e-02 ]
</pre></div>
</div>
</div>
<p>The analysis driver script (the script being driven by Dakota) has to
perform several tasks in the case of parameter estimation using
nonlinear least-squares methods. The analysis driver script must: (1)
read in the values of the parameters supplied by Dakota; (2) run the
computer simulation with these parameter values; (3) retrieve the
results from the computer simulation; (4) compute the difference between
each computed simulation value and the corresponding experimental or
measured value; and (5) write these residuals (differences) to an
external file that gets passed back to Dakota. Note there will be one
line per residual term, specified with <code class="docutils literal notranslate"><span class="pre">calibration_terms</span></code> in the
Dakota input file. It is the last two steps which are different from
most other Dakota applications.</p>
<p>To simplify specifying a least squares problem, one may provide Dakota a
data file containing experimental results or other calibration data. In
the case of scalar calibration terms, this file may be specified with .
In this case, Dakota will calculate the residuals (that is, the
simulation model results minus the experimental results), and the
user-provided script can omit this step: the script can just return the
simulation outputs of interest. An example of this can be found in the
file named <code class="docutils literal notranslate"><span class="pre">dakota/share/dakota/examples/users/textbook_nls_datafile.in</span></code>.
In this example, there are 3 residual terms. The data file
of experimental results associated with this example is
<code class="docutils literal notranslate"><span class="pre">textbook_nls_datafile.lsq.dat</span></code>. These three
values are subtracted from the least-squares terms to produce residuals
for the nonlinear least-squares problem. Note that the file may be
annotated (specified by <code class="docutils literal notranslate"><span class="pre">annotated</span></code>) or freeform (specified by
<code class="docutils literal notranslate"><span class="pre">freeform</span></code>). The number of experiments in the calibration data file
may be specified with , with one row of data per experiment. When
multiple experiments are present, the total number of least squares
terms will be the number of calibration terms times the number of
experiments.</p>
<p>Finally, the calibration data file may contain additional information
than just the observed experimental responses. If the observed data has
measurement error associated with it, this can be specified in columns
of such error data after the response data. The type of measurement
error is specified by <code class="docutils literal notranslate"><span class="pre">variance_type</span></code>. For scalar calibration terms,
the <code class="docutils literal notranslate"><span class="pre">variance_type</span></code> can be either <code class="docutils literal notranslate"><span class="pre">none</span></code> (the user does not specify
a measurement variance associated with each calibration term) or
<code class="docutils literal notranslate"><span class="pre">scalar</span></code> (the user specifies one measurement variance per calibration
term). For field calibration terms, the <code class="docutils literal notranslate"><span class="pre">variance_type</span></code> can also be
<code class="docutils literal notranslate"><span class="pre">diagonal</span></code> or <code class="docutils literal notranslate"><span class="pre">matrix</span></code>. These are explained in more detail in the
Reference manual. See the <a class="reference internal" href="../reference.html#keyword-reference-area"><span class="std std-ref">Keyword Reference</span></a>
for more information. Additionally, there is sometimes the need to specify
configuration variables. These are often used in Bayesian calibration
analysis. These are specified as <code class="docutils literal notranslate"><span class="pre">num_config_variables</span></code>. If the user
specifies a positive number of configuration variables, it is expected
that they will occur in the text file before the responses.</p>
</section>
<section id="usage-guidelines">
<span id="nls-usage"></span><h2>Usage Guidelines<a class="headerlink" href="#usage-guidelines" title="Link to this heading"></a></h2>
<p>Calibration problems can be transformed to general optimization problems
where the objective is some type of aggregated error metric. For
example, the objective could be the sum of squared error terms. However,
it also could be the mean of the absolute value of the error terms, the
maximum difference between the simulation results and observational
results, etc. In all of these cases, one can pose the calibration
problem as an optimization problem that can be solved by any of Dakota’s
optimizers. In this situation, when applying an general optimization
solver to a calibration problem, the guidelines in
Guide <a class="reference internal" href="optimization.html#opt-usage-guideopt"><span class="std std-numref">Table 12</span></a> still apply.</p>
<p>In some cases, it will be better to use a nonlinear least-squares method
instead of a general optimizer to determine optimal parameter values
which result in simulation responses that “best fit” the observational
data. Nonlinear least squares methods exploit the special structure of a
sum of the squares objective function. They can be much more efficient
than general optimizers. However, these methods require the gradients of
the function with respect to the parameters being calibrated. If the
model is not able to produce gradients, one can use finite differencing
to obtain gradients. However, the gradients must be reasonably accurate
for the method to proceed. Note that the nonlinear least-squares methods
only operate on a sum of squared errors as the objective. Also, the user
must return each residual term separately to Dakota, whereas the user
can return an aggregated error measure in the case of general
optimizers.</p>
<p>The three nonlinear least-squares methods are the Gauss-Newton method in
OPT++, NLSSOL, and NL2SOL. Any of these may be tried; they give similar
performance on many problems. NL2SOL tends to be more robust than
Gauss-Newton, especially for nonlinear functions and large-residual
problems where one is not able to drive the residuals to zero at the
solution. NLSSOL does require that the user has the NPSOL library. Note
that all of these methods are local in the sense that they are
gradient-based and depend on an initial starting point. Often they are
used in conjunction with a multi-start method, to perform several
repetitions of the optimization at different starting points in the
parameter space. Another approach is to use a general global optimizer
such as a genetic algorithm or DIRECT as mentioned above. This can be
much more expensive, however, in terms of the number of function
evaluations required.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="optimization.html" class="btn btn-neutral float-left" title="Optimization" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../topics.html" class="btn btn-neutral float-right" title="Topics" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <!--
  <div role="contentinfo">
    <p>&#169; Copyright 2024, Sandia National Laboratories.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
  
  --> 

</footer>
        </div>
      </div>
	  
	  <div style="background-color: #0f0f0f;color:#fafafa;padding:20px">
	    <div>
		  <h2><em>Exceptional service in the national interest</em></h2>
		</div>
		<p>© 2023 National Technology and Engineering Solutions of Sandia, LLC. | <a href="https://www.sandia.gov/contact_us/index.html">Questions &amp; Comments</a> | <a href="https://www.sandia.gov/general/privacy-security/index.html">Privacy &amp; Security</a></p>
		<p><a href="http://energy.gov" rel="noopener noreferrer" target="_blank"><img alt="U.S. Department of Energy" longdesc="https://energy.gov" src="https://www.sandia.gov/_common/images/doe_logo_white.png" style="height:37px; width:140px"></a> <a href="http://nnsa.energy.gov/" rel="noopener noreferrer" target="_blank"> <img alt="National Nuclear Security Administration" longdesc="http://nnsa.gov" src="https://www.sandia.gov/_common/images/nnsa_logo_white.png" style="height:37px; width:116px"></a></p>
		<p><a href="https://www.sandia.gov">Sandia National Laboratories</a> is a multimission laboratory managed and operated by National Technology and Engineering Solutions of Sandia, LLC., a wholly owned subsidiary of Honeywell International, Inc., for the U.S. Department of Energy’s National Nuclear Security Administration under contract DE-NA-0003525.</p>
	  </div>	  	  
	  
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>