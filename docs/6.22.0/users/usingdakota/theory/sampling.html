<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Sampling Methods &mdash; dakota  documentation</title>
      <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/dakota_theme.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/sandiaheaderlite.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../../_static/jquery.js?v=5d32c60e"></script>
        <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../../_static/documentation_options.js?v=5929fcd5"></script>
        <script src="../../_static/doctools.js?v=9a2dae69"></script>
        <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
        <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
        <script src="../../_static/copybutton.js?v=f281be69"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Reliability Methods" href="reliability.html" />
    <link rel="prev" title="Dakota Theory" href="../theory.html" /> 
  
  <meta name="sandia.approval_type" content="formal"/>
  <meta property="sandia.approved" content="SAND2025-05563O"/>
  <meta name="description" content="The Dakota project delivers both state-of-the-art research and robust, usable software for optimization and UQ."/>
  <meta name="keywords" content="Dakota, optimization, UQ, uncertainty quantification, parametric analysis, design exploration, model calibration, risk analysis"/>
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> dakota
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../setupdakota.html">Getting Started</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../usingdakota.html">Using Dakota</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../introduction/aboutdakota.html">About Dakota</a></li>
<li class="toctree-l2"><a class="reference internal" href="../introduction/helloworld.html">Dakota Beginner’s Tutorial</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples.html">Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../introduction/couplingtosimulations.html">Coupling Dakota to a Simulation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../inputfile.html">Dakota Input File</a></li>
<li class="toctree-l2"><a class="reference internal" href="../running.html">Running Dakota</a></li>
<li class="toctree-l2"><a class="reference internal" href="../output.html">Dakota Output</a></li>
<li class="toctree-l2"><a class="reference internal" href="../studytypes.html">Study Types</a></li>
<li class="toctree-l2"><a class="reference internal" href="../topics.html">Topics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../advanced.html">Advanced Topics</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../theory.html">Dakota Theory</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">Sampling Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#monte-carlo-mc">Monte Carlo (MC)</a></li>
<li class="toctree-l4"><a class="reference internal" href="#multifidelity-monte-carlo">Multifidelity Monte Carlo</a></li>
<li class="toctree-l4"><a class="reference internal" href="#multilevel-monte-carlo">Multilevel Monte Carlo</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#multilevel-monte-carlo-for-the-mean">Multilevel Monte Carlo for the mean</a></li>
<li class="toctree-l5"><a class="reference internal" href="#mlmc-extension-to-the-variance">MLMC extension to the variance</a></li>
<li class="toctree-l5"><a class="reference internal" href="#mlmc-extension-to-the-standard-deviation">MLMC extension to the standard deviation</a></li>
<li class="toctree-l5"><a class="reference internal" href="#mlmc-extension-to-the-scalarization-function">MLMC extension to the scalarization function</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="#a-multilevel-multifidelity-approach">A multilevel-multifidelity approach</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#y-l-correlations"><span class="math notranslate nohighlight">\(Y_l\)</span> correlations</a></li>
<li class="toctree-l5"><a class="reference internal" href="#q-l-correlations"><span class="math notranslate nohighlight">\(Q_l\)</span> correlations</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="#quasi-monte-carlo-qmc">Quasi-Monte Carlo (QMC)</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#rank-1-lattice-rules-and-sequences">Rank-1 lattice rules and sequences</a></li>
<li class="toctree-l5"><a class="reference internal" href="#digital-nets-and-sequences">Digital nets and sequences</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="reliability.html">Reliability Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="stochastic.html">Stochastic Expansion Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="epistemic.html">Epistemic Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="bayesian.html">Bayesian Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="surrogates.html">Surrogate Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="surrogatebasedoptimization.html">Surrogate-Based Local Minimization</a></li>
<li class="toctree-l3"><a class="reference internal" href="surrogatebasedglobaloptimization.html">Efficient Global Optimization</a></li>
<li class="toctree-l3"><a class="reference internal" href="dimensionreductionstrategies.html">Dimension Reduction Strategies</a></li>
<li class="toctree-l3"><a class="reference internal" href="ouu.html">Optimization Under Uncertainty (OUU)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../reference.html">Keyword Reference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../usingdakotagui/usingdakotagui.html">Using Dakota GUI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../externaltools/externaltools.html">Using External Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../compiling/compiling.html">Compiling Dakota</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../developingdakota/developingdakota.html">Developing Dakota</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../misc/misc.html">Miscellaneous</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">dakota</a>
      </nav>

	  <!-- SNL Lite header -->
	  <div class="snlheader-subsite--wrapper-default">
		<snlheader class="snlheader-subsite" role="snlbanner">
		  <div class="wrapper">
			<a href="https://www.sandia.gov/index.html">
			  <div class="logo-transparent"><p class="logo">Sandia National Laboratories</p></div>
			</a>
			<div class="nav-top">
			  <a class="visuallyhidden" name="mainnav"></a>
			  <div aria-label="main navigation" class="core-nav-transparent core-nav-transparent--visible" role="navigation">
				<ul role="navigation" class="secondary-links">
				  <li id="search-text-link">
					<a aria-label="Search" href="https://www.sandia.gov/search/">Search Sandia.gov</a>
				  </li>
				  <li id="directory-text-link">
					<a href="https://www.sandia.gov/directory.html" aria-expanded="false" aria-label="Site Directory">All Sandia Websites</a>
				  </li>
				</ul>
			  </div>
			</div>
		  </div> 
		</snlheader>
	  </div>	  

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../usingdakota.html">Using Dakota</a></li>
          <li class="breadcrumb-item"><a href="../theory.html">Dakota Theory</a></li>
      <li class="breadcrumb-item active">Sampling Methods</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/usingdakota/theory/sampling.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="sampling-methods">
<span id="sampling-theory-main"></span><h1>Sampling Methods<a class="headerlink" href="#sampling-methods" title="Link to this heading"></a></h1>
<p>This chapter introduces several fundamental concepts related to sampling
methods. In particular, the statistical properties of the Monte Carlo
estimator are discussed (<a class="reference internal" href="#uq-sampling-montecarlo"><span class="std std-ref">Monte Carlo (MC)</span></a>) and
strategies for multilevel and multifidelity sampling are introduced
within this context. Hereafter, multilevel refers to the possibility of
exploiting distinct discretization levels (i.e. space/time resolution)
within a single model form, whereas multifidelity involves the use of
more than one model form. In <a class="reference internal" href="#uq-sampling-mfmc"><span class="std std-ref">Multifidelity Monte Carlo</span></a>,
we describe the multifidelity Monte Carlo and its single fidelity model version, the control variate Monte Carlo,
that we align with
multifidelity sampling, and in <a class="reference internal" href="#uq-sampling-multilevel"><span class="std std-ref">Multilevel Monte Carlo</span></a>, we
describe the multilevel Monte Carlo algorithm that we align with
multilevel sampling. In <a class="reference internal" href="#uq-sampling-mlmf"><span class="std std-ref">A multilevel-multifidelity approach</span></a>, we show that
these two approaches can be combined to create multilevel-multifidelity
sampling approaches. Finally, this chapter discusses <a class="reference internal" href="#uq-sampling-quasimontecarlo"><span class="std std-ref">Quasi-Monte Carlo (QMC)</span></a>
or low-discrepancy sampling.</p>
<section id="monte-carlo-mc">
<span id="uq-sampling-montecarlo"></span><h2>Monte Carlo (MC)<a class="headerlink" href="#monte-carlo-mc" title="Link to this heading"></a></h2>
<p>Monte Carlo is a popular algorithm for stochastic simulations due to its
simplicity, flexibility, and the provably convergent behavior that is
independent of the number of input uncertainties. A quantity of interest
<span class="math notranslate nohighlight">\(Q: \Xi \rightarrow \mathbb{R}\)</span>, represented as a random variable
(RV), can be introduced as a function of a random vector
<span class="math notranslate nohighlight">\(\boldsymbol{\xi} \in \Xi \subset \mathbb{R}^d\)</span>. The goal of any
MC simulation is computing statistics for <span class="math notranslate nohighlight">\(Q\)</span>, e.g. the expected
value <span class="math notranslate nohighlight">\(\mathbb{E}\left[Q\right]\)</span>. The MC estimator
<span class="math notranslate nohighlight">\(\hat{Q}_N^{MC}\)</span> for <span class="math notranslate nohighlight">\(\mathbb{E}\left[Q\right]\)</span> is defined
as follows</p>
<div class="math notranslate nohighlight" id="equation-mc">
<span class="eqno">(37)<a class="headerlink" href="#equation-mc" title="Link to this equation"></a></span>\[\hat{Q}_N^{MC} = \dfrac{1}{N} \sum_{i=1}^N Q^{(i)},\]</div>
<p>where <span class="math notranslate nohighlight">\(Q^{(i)} = Q(\boldsymbol{\xi}^{(i)})\)</span> and <span class="math notranslate nohighlight">\(N\)</span> is used
to indicate the number of realizations of the model.</p>
<p>The MC estimator is unbiased, i.e., its bias is zero and its convergence to the true
statistics is <span class="math notranslate nohighlight">\(\mathcal{O}(N^{-1/2})\)</span>. Moreover, since each
set of realizations for <span class="math notranslate nohighlight">\(Q\)</span> is different, another crucial property of any
estimator is its own variance:</p>
<div class="math notranslate nohighlight" id="equation-variancemc">
<span class="eqno">(38)<a class="headerlink" href="#equation-variancemc" title="Link to this equation"></a></span>\[\mathbb{V}ar\left( \hat{Q}_N^{MC} \right) = \dfrac{\mathbb{V}ar\left(Q\right) }{N}.\]</div>
<p>Furthermore, it is possible to show, in the limit
<span class="math notranslate nohighlight">\(N \rightarrow \infty\)</span>, that the error
<span class="math notranslate nohighlight">\(\left( \mathbb{E}\left[Q\right] - \hat{Q}_N^{MC} \right) \sim
\sqrt{\dfrac{\mathbb{V}ar\left(Q\right) }{N}} \mathcal{N}(0,1)\)</span>, where
<span class="math notranslate nohighlight">\(\mathcal{N}(0,1)\)</span> represents a standard normal RV. As a
consequence, it is possible to define a confidence interval for the MC
estimator which has an amplitude proportional to the standard deviation
of the estimator. Indeed, the variance of the estimator plays a
fundamental role in the quality of the numerical results: the reduction
of the estimator variance correspond to an error reduction in the statistics.</p>
</section>
<section id="multifidelity-monte-carlo">
<span id="uq-sampling-mfmc"></span><h2>Multifidelity Monte Carlo<a class="headerlink" href="#multifidelity-monte-carlo" title="Link to this heading"></a></h2>
<p>A closer inspection of Eq. <a class="reference internal" href="#equation-variancemc">(38)</a>
indicates that only an increase in the number of simulations <span class="math notranslate nohighlight">\(N\)</span>
might reduce the overall variance, since
<span class="math notranslate nohighlight">\(\mathbb{V}ar\left({Q}\right)\)</span> is an intrinsic property of the
model under analysis. However, more sophisticated techniques have been
proposed to accelerate the convergence of a MC simulation. For instance,
an incomplete list of these techniques can include stratified sampling,
importance sampling, Latin hypercube, deterministic Sobol’ sequences and
control variates (see <span id="id1">[<a class="reference internal" href="../../misc/bibliography.html#id227" title="Art B. Owen. Monte Carlo theory, methods and examples. Art Owen, 2013. URL: https://artowen.su.domains/mc/.">Owe13</a>]</span>). In particular, the control variate approach, is based
on the idea of replacing the RV <span class="math notranslate nohighlight">\(Q\)</span> with one that has
the same expected value, but with a smaller variance. The goal is to
reduce the numerator in Eq. <a class="reference internal" href="#equation-variancemc">(38)</a>,
and hence the value of the estimator variance without requiring a larger
number of simulations. In a practical setting, the control variate makes
use of an auxiliary RV <span class="math notranslate nohighlight">\(G=G(\boldsymbol{\xi})\)</span> for which
the expected value <span class="math notranslate nohighlight">\(\mathbb{E}\left[G\right]\)</span> is known. Indeed,
the alternative estimator can be defined as</p>
<div class="math notranslate nohighlight" id="equation-control-variate">
<span class="eqno">(39)<a class="headerlink" href="#equation-control-variate" title="Link to this equation"></a></span>\[\hat{Q}_N^{MCCV} =  \hat{Q}_N^{MC} - \beta \left( \hat{G}_N^{MC} - \mathbb{E}\left[G\right] \right), \quad \mathrm{where} \quad \beta \in \mathbb{R}.\]</div>
<p>The MC control variate estimator <span class="math notranslate nohighlight">\(\hat{Q}_N^{MCCV}\)</span> is unbiased, but its variance now has a more complex
dependence not only on the <span class="math notranslate nohighlight">\(\mathbb{V}ar\left({Q}\right)\)</span>, but
also on <span class="math notranslate nohighlight">\(\mathbb{V}ar\left(G\right)\)</span> and the covariance between
<span class="math notranslate nohighlight">\(Q\)</span> and <span class="math notranslate nohighlight">\(G\)</span> since</p>
<div class="math notranslate nohighlight">
\[\mathbb{V}ar\left(\hat{Q}_N^{MCCV}\right) = \dfrac{1}{N} \left( \mathbb{V}ar\left( \hat{Q}_N^{MC} \right) + \beta^2 \mathbb{V}ar\left( \hat{G}_N^{MC} \right) - 2\beta \mathrm{Cov}\left(Q,G\right) \right).\]</div>
<p>The parameter <span class="math notranslate nohighlight">\(\beta\)</span> can be used to minimize the overall variance
leading to</p>
<div class="math notranslate nohighlight">
\[\beta = \dfrac{ \mathrm{Cov}\left(Q,G\right) }{ \mathbb{V}ar\left( G \right) },\]</div>
<p>for which the estimator variance follows as</p>
<div class="math notranslate nohighlight">
\[\mathbb{V}ar\left({\hat{Q}_N^{MCCV}}\right) = \mathbb{V}ar\left({\hat{Q}_N^{MC}}\right)\left( 1-\rho^2 \right).\]</div>
<p>Therefore, the overall variance of the estimator
<span class="math notranslate nohighlight">\(\hat{Q}_N^{MCCV}\)</span> is proportional to the variance of the standard
MC estimator <span class="math notranslate nohighlight">\(\hat{Q}_N^{MC}\)</span> through a factor <span class="math notranslate nohighlight">\(1-\rho^2\)</span>
where
<span class="math notranslate nohighlight">\(\rho = \dfrac{ \mathrm{Cov}\left(Q,G\right) }{\sqrt{\mathbb{V}ar\left(Q\right)\mathbb{V}ar\left(G\right)}}\)</span>
is the Pearson correlation coefficient between <span class="math notranslate nohighlight">\(Q\)</span> and <span class="math notranslate nohighlight">\(G\)</span>.
Since <span class="math notranslate nohighlight">\(0&lt;\rho^2&lt;1\)</span>, the variance
<span class="math notranslate nohighlight">\(\mathbb{V}ar\left( \hat{Q}_N^{MCCV} \right)\)</span> is always less than
the corresponding <span class="math notranslate nohighlight">\(\mathbb{V}ar\left({\hat{Q}_N^{MC}}\right)\)</span>. The
control variate technique can be seen as a very general approach to
accelerate a MC simulation. The main step is to define a convenient
control variate function which is cheap to evaluate and well correlated
to the target function. For instance, function evaluations obtained
through a different (coarse) resolution may be employed or even coming
from a more crude physical/engineering approximation of the problem. A
viable way of building a well correlated control variate is to rely on a
low-fidelity model (i.e. a crude approximation of the model of interest)
to estimate the control variate using estimated control means (see
<span id="id2">[<a class="reference internal" href="../../misc/bibliography.html#id324" title="L. W. T. Ng. and K. E. Willcox. Multifidelity approaches for optimization under uncertainty. International Journal for numerical methods in Engineering, 100(10):746–772, 2014.">NgW14</a>, <a class="reference internal" href="../../misc/bibliography.html#id323" title="R. Pasupathy, M. Taaffe, B. W. Schmeiser, and W. Wang. Control-variate estimation using estimated control means. IIE Transactions, 44(5):381–385, 2014.">PTSW14</a>]</span> for more details). In this latter case,
clearly the expected value of the low-fidelity model is not known and needs to be computed.</p>
<p>With a slight change in notation, it is possible to write</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\hat{Q}^{CVMC} = \hat{Q} + \alpha_1 \left( \hat{Q}_1 - \hat{\mu}_1 \right),\]</div>
</div></blockquote>
<p>where <span class="math notranslate nohighlight">\(\hat{Q}\)</span> represents the MC estimator for the high-fidelity model, <span class="math notranslate nohighlight">\(\hat{Q}_1\)</span> the MC estimator for the low-fidelity model
and <span class="math notranslate nohighlight">\(\hat{\mu}_1\)</span> a different approximation for <span class="math notranslate nohighlight">\(\mathbb{E}[Q_1]\)</span>. If <span class="math notranslate nohighlight">\(N\)</span> samples are used for approximating <span class="math notranslate nohighlight">\(\hat{Q}\)</span> and
<span class="math notranslate nohighlight">\(\hat{Q}_1\)</span> and a total of <span class="math notranslate nohighlight">\(r_1 N\)</span> samples for the low-fidelity models are available, an optimal solution, which guarantees the best use of the low-fidelity resources,
can be obtained following <span id="id3">[<a class="reference internal" href="../../misc/bibliography.html#id324" title="L. W. T. Ng. and K. E. Willcox. Multifidelity approaches for optimization under uncertainty. International Journal for numerical methods in Engineering, 100(10):746–772, 2014.">NgW14</a>]</span> as</p>
<div class="math notranslate nohighlight">
\[\alpha_1 = -\rho_1 \sqrt{ \frac{ \mathbb{V}ar[Q] }{ \mathbb{V}ar[Q_1] } }\]</div>
<div class="math notranslate nohighlight">
\[r_1 = \sqrt{ \frac{ \mathcal{C} }{ \mathcal{C}_1 } \frac{\rho_1^2}{1-\rho_1^2} },\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathcal{C}\)</span> and <span class="math notranslate nohighlight">\(\mathcal{C}_1\)</span> represent the cost of evaluating the high- and low-fidelity models, respectively and <span class="math notranslate nohighlight">\(\rho_1\)</span> is the correlation between the two models. This solution leads to the following expression for the estimator variance</p>
<div class="math notranslate nohighlight">
\[\mathbb{V}ar[\hat{Q}^{CVMC}] = \mathbb{V}ar[\hat{Q}] \left( 1 - \frac{r_1-1}{r_1} \rho_1^2 \right),\]</div>
<p>which shows similarities with the variance of a control variate estimator with the only difference being the term <span class="math notranslate nohighlight">\(\frac{r_1-1}{r_1}\)</span> that, by multiplying the correlation
<span class="math notranslate nohighlight">\(\rho_1\)</span>, effectively penalizes the estimator due to the need for estimating the low-fidelity mean.</p>
<p>Another common case encountered in practice is the availability of more than a low-fidelity model. In this case, the multifidelity Monte Carlo can be extended following
<span id="id4">[<a class="reference internal" href="../../misc/bibliography.html#id214" title="Benjamin Peherstorfer, Karen Willcox, and Max Gunzburger. Optimal model management for multifidelity monte carlo estimation. SIAM Journal on Scientific Computing, 38(5):A3163–A3194, 2016.">PWG16</a>, <a class="reference internal" href="../../misc/bibliography.html#id215" title="Benjamin Peherstorfer, Karen Willcox, and Max Gunzburger. Survey of Multifidelity Methods in Uncertainty Propagation, Inference, and Optimization. SIAM Review, 60(3):550–591, January 2018. URL: https://epubs.siam.org/doi/10.1137/16M1082469 (visited on 2019-10-04), doi:10.1137/16M1082469.">PWG18</a>]</span> as</p>
<div class="math notranslate nohighlight">
\[\hat{Q}^{MFMC} = \hat{Q} + \sum_{i=1}^M \alpha_i \left( \hat{Q}_i - \hat{\mu}_i \right),\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{Q}_i\)</span> represents the generic ith low-fidelity model.</p>
<p>The MFMC estimator is still unbiased (similarly to MC) and share similarities with CVMC; indeed one can recover CVMC directly from it. For each low-fidelity model we use <span class="math notranslate nohighlight">\(N_i r_i\)</span> samples, as in the CVMC case, however for <span class="math notranslate nohighlight">\(i \geq 2\)</span>, the term <span class="math notranslate nohighlight">\(\hat{Q_i}\)</span> is approximated with exactly the same samples of the previous model, while each <span class="math notranslate nohighlight">\(\hat{\mu}_i\)</span> is obtained by adding to this set a number of <span class="math notranslate nohighlight">\((r_i-r_{i-1}) N_i\)</span> additional independent samples. Following <span id="id5">[<a class="reference internal" href="../../misc/bibliography.html#id214" title="Benjamin Peherstorfer, Karen Willcox, and Max Gunzburger. Optimal model management for multifidelity monte carlo estimation. SIAM Journal on Scientific Computing, 38(5):A3163–A3194, 2016.">PWG16</a>]</span> the weights can be obtained as</p>
<div class="math notranslate nohighlight" id="equation-mfmc-alpha">
<span class="eqno">(40)<a class="headerlink" href="#equation-mfmc-alpha" title="Link to this equation"></a></span>\[\alpha_i = - \rho_i \sqrt{ \frac{ \mathbb{V}ar[Q] }{ \mathbb{V}ar[Q_i] } }.\]</div>
<p>The optimal resource allocation problem is also obtainable in closed-form if, as demonstrated in <span id="id6">[<a class="reference internal" href="../../misc/bibliography.html#id214" title="Benjamin Peherstorfer, Karen Willcox, and Max Gunzburger. Optimal model management for multifidelity monte carlo estimation. SIAM Journal on Scientific Computing, 38(5):A3163–A3194, 2016.">PWG16</a>]</span> the following conditions, for the models’ correlations and costs, hold</p>
<div class="math notranslate nohighlight">
\[|\rho_1| &gt; |\rho_2| &gt; \dots &gt; |\rho_M|\]</div>
<div class="math notranslate nohighlight">
\[\frac{\mathcal{C}_{i-1}}{\mathcal{C}_{i}} &gt; \frac{ \rho_{i-1}^2 - \rho_{i}^2 }{ \rho_{i}^2 - \rho_{i+1}^2 },\]</div>
<p>leading to</p>
<div class="math notranslate nohighlight">
\[r_i = \sqrt{ \frac{\mathcal{C}}{\mathcal{C}_i} \frac{\rho_i^2 - \rho_{i+1}^2}{1-\rho_1^2} }.\]</div>
</section>
<section id="multilevel-monte-carlo">
<span id="uq-sampling-multilevel"></span><h2>Multilevel Monte Carlo<a class="headerlink" href="#multilevel-monte-carlo" title="Link to this heading"></a></h2>
<p>In general engineering applications, the quantity of interest <span class="math notranslate nohighlight">\(Q\)</span>
is obtained as the result of the numerical solution of a partial partial
differential equation (possibly a system of them). Therefore, the
dependence on the physical
<span class="math notranslate nohighlight">\(\mathbf{x} \in \Omega\subset\mathbb{R}^n\)</span> and/or temporal
<span class="math notranslate nohighlight">\(t \in T\subset\mathbb{R^+}\)</span> coordinates should be included, hence
<span class="math notranslate nohighlight">\(Q=Q(\mathbf{x}, \boldsymbol{\xi}, t)\)</span>. A finite spatial/temporal
resolution is always employed to numerically solve a PDE, implying the
presence of a discretization error in addition to the stochastic error.
The term discretization is applied generically with reference to either
the spatial tessellation, the temporal resolution, or both (commonly,
they are linked). For a generic tessellation with <span class="math notranslate nohighlight">\(M\)</span>
degrees-of-freedom (DOFs), the PDE solution of <span class="math notranslate nohighlight">\(Q\)</span> is referred to
as <span class="math notranslate nohighlight">\(Q_M\)</span>. Since <span class="math notranslate nohighlight">\(Q_M \rightarrow Q\)</span> for
<span class="math notranslate nohighlight">\(M\rightarrow\infty\)</span>, then
<span class="math notranslate nohighlight">\(\mathbb{E}\left[{Q_M}\right] \rightarrow \mathbb{E}\left[{Q}\right]\)</span>
for <span class="math notranslate nohighlight">\(M\rightarrow\infty\)</span> with a prescribed order of convergence. A
MC estimator in presence of a finite spatial resolution and finite
sampling is</p>
<div class="math notranslate nohighlight">
\[\hat{Q}^{MC}_{M,N} = \frac{1}{N} \sum_{i=1}^N Q_M^{(i)}\]</div>
<p>for which the mean square error (MSE) is</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}\left[ (\hat{Q}^{MC}_{M,N}-\mathbb{E}\left[ Q \right] )^2 \right]
       = N^{-1} \mathbb{V}ar\left({Q_M}\right) + \left( \mathbb{E}\left[{ Q_M-Q }\right] \right)^2,\]</div>
<p>where the first term represents the variance of the estimator, and the
second term <span class="math notranslate nohighlight">\(\left( \mathbb{E}\left[ Q_M-Q \right] \right)^2\)</span>
reflects the bias introduced by the (finite) spatial discretization. The
two contributions appear to be independent of each other; accurate MC
estimates can only be obtained by drawing the required <span class="math notranslate nohighlight">\(N\)</span> number
of simulations of <span class="math notranslate nohighlight">\(Q_M( \boldsymbol{\xi} )\)</span> at a sufficiently fine
resolution <span class="math notranslate nohighlight">\(M\)</span>. Since the numerical cost of a PDE is related to
the number of DOFs of the tessellation, the total cost of a MC
simulation for a PDE can easily become intractable for complex
multi-physics applications that are computationally intensive.</p>
<section id="multilevel-monte-carlo-for-the-mean">
<h3>Multilevel Monte Carlo for the mean<a class="headerlink" href="#multilevel-monte-carlo-for-the-mean" title="Link to this heading"></a></h3>
<p>The multilevel Monte Carlo (MLMC) algorithm has been introduced,
starting from the control variate idea, for situation in which
additional discretization levels can be defined. The basic idea,
borrowed from the multigrid approach, is to replace the evaluation of
the statistics of <span class="math notranslate nohighlight">\(Q_M\)</span> with a sequence of evaluations at coarser
levels. If it is possible to define a sequence of discretization levels
<span class="math notranslate nohighlight">\(\left\{ M_\ell: \ell = 0, \dots, L \right\}\)</span> with
<span class="math notranslate nohighlight">\(M_0 &lt; M_1 &lt; \dots &lt; M_L \stackrel{\mathrm{def}}{=} M\)</span>, the
expected value <span class="math notranslate nohighlight">\(\mathbb{E}\left[{Q_M}\right]\)</span> can be decomposed,
exploiting the linearity of the expected value operator as</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}\left[{Q_{M}}\right] = \mathbb{E}\left[{Q_{M_0}}\right] + \sum_{\ell = 1}^L \mathbb{E }\left[ Q_{M_{\ell}} - Q_{M_{\ell-1}} \right].\]</div>
<p>If the difference function <span class="math notranslate nohighlight">\(Y_\ell\)</span> is defined according to</p>
<div class="math notranslate nohighlight">
\[\begin{split}Y_\ell = \left\{
 \begin{split}
 Q_{M_0} \quad &amp;\mathrm{if} \quad \ell=0 \\
 Q_{M_{\ell}} - Q_{M_{\ell-1}} \quad &amp;\mathrm{if} \quad 0&lt;\ell\leq L,
 \end{split}
 \right.\end{split}\]</div>
<p>the expected value
<span class="math notranslate nohighlight">\(\mathbb{E}\left[{Q_M}\right]=\sum_{\ell=0}^{L}{  \mathbb{E}\left[Y_\ell\right]   }\)</span>.
A multilevel MC estimator is obtained when a MC estimator is adopted
independently for the evaluation of the expected value of <span class="math notranslate nohighlight">\(Y_\ell\)</span>
on each level. The resulting multilevel estimator
<span class="math notranslate nohighlight">\(\hat{Q}_M^{\mathrm{ML}}\)</span> is</p>
<div class="math notranslate nohighlight">
\[\hat{Q}_M^{\mathrm{ML}} = \, \sum_{\ell = 0}^L \hat{Y}_{\ell, N_\ell}^{\mathrm{MC}}
 = \sum_{\ell = 0}^L \frac{1}{N_\ell} \sum_{i=1}^{N_\ell} Y_\ell^{(i)}.\]</div>
<p>Since the multilevel estimator is unbiased, the advantage of using this
formulation is in its reduced estimator variance
<span class="math notranslate nohighlight">\(\sum_{\ell=0}^{L} N_\ell^{-1} \mathbb{V}ar\left({Y_\ell}\right)\)</span>:
since <span class="math notranslate nohighlight">\(Q_M \rightarrow Q\)</span>, the difference function
<span class="math notranslate nohighlight">\(Y_\ell \rightarrow 0\)</span> as the level <span class="math notranslate nohighlight">\(\ell\)</span> increases.
Indeed, the corresponding number of samples <span class="math notranslate nohighlight">\(N_\ell\)</span> required to
resolve the variance associated with the <span class="math notranslate nohighlight">\(\ell\)</span>th level is
expected to decrease with <span class="math notranslate nohighlight">\(\ell\)</span>.</p>
<p>The MLMC algorithm can be interpreted as a strategy to optimally
allocate resources. If the total cost of the MLMC algorithm is written
as</p>
<div class="math notranslate nohighlight">
\[\label{EQ: MLMC cost}
\mathcal{C}(\hat{Q}^{ML}_{M}) = \sum_{\ell=0}^{L} N_\ell \, \mathcal{C}_{\ell},\]</div>
<p>with <span class="math notranslate nohighlight">\(\mathcal{C}_{\ell}\)</span> being the cost of the evaluation of
<span class="math notranslate nohighlight">\(Y_\ell\)</span> (involving either one or two discretization evaluations),
then the following constrained minimization problem can be formulated
where an equality constraint enforces a stochastic error (from MLMC
estimator variance) equal to the residual bias error
(<span class="math notranslate nohighlight">\(\varepsilon^2/2\)</span>)</p>
<div class="math notranslate nohighlight" id="equation-mlmc-optimization">
<span class="eqno">(41)<a class="headerlink" href="#equation-mlmc-optimization" title="Link to this equation"></a></span>\[ f(N_\ell,\lambda) = \sum_{\ell=0}^{L} N_\ell \, \mathcal{C}_{\ell}
                   + \lambda \left( \sum_{\ell=0}^{L} N_\ell^{-1} \mathbb{V}ar\left({Y_\ell}\right) - \varepsilon^2/2 \right).\]</div>
<p>using a Lagrange multiplier <span class="math notranslate nohighlight">\(\lambda\)</span>. This equality constraint
reflects a balance between the two contributions to MSE, reflecting the
goal to not over-resolve one or the other. The result of the
minimization is</p>
<div class="math notranslate nohighlight">
\[\label{EQ: MLMC nl}
N_{\ell} = \frac{2}{\varepsilon^2} \left[ \, \sum_{k=0}^L \left( \mathbb{V}ar\left(Y_k\right) \mathcal{C}_k \right)^{1/2} \right]
               \sqrt{\frac{ \mathbb{V}ar\left({Y_\ell}\right) }{\mathcal{C}_{\ell}}},\]</div>
<p>defining an optimal sample allocation per discretization level.</p>
</section>
<section id="mlmc-extension-to-the-variance">
<h3>MLMC extension to the variance<a class="headerlink" href="#mlmc-extension-to-the-variance" title="Link to this heading"></a></h3>
<p>Despite the original introduction of the MLMC approach for the
computation of the mean estimator in
<span id="id7">[<a class="reference internal" href="../../misc/bibliography.html#id115" title="Michael B. Giles. Multilevel Monte Carlo Path Simulation. Operations Research, 56(3):607–617, June 2008. URL: http://pubsonline.informs.org/doi/abs/10.1287/opre.1070.0496, doi:10.1287/opre.1070.0496.">Gil08</a>, <a class="reference internal" href="../../misc/bibliography.html#id116" title="Michael B. Giles. Multilevel monte carlo methods. Acta Numerica, 24:259–328, 2015. doi:10.1017/S096249291500001X.">Gil15</a>]</span>, it is possible to estimate
higher-order moments with a MLMC sampling strategy, as for instance the
variance.</p>
<p>A single level unbiased estimator for the variance of a generic QoI at
the highest level <span class="math notranslate nohighlight">\(M_L\)</span> of the hierarchy can be written as</p>
<div class="math notranslate nohighlight" id="equation-variance-est-single-level">
<span class="eqno">(42)<a class="headerlink" href="#equation-variance-est-single-level" title="Link to this equation"></a></span>\[\mathbb{V}ar\left[Q_{M_L}\right] \approx \frac{1}{N_{M_L} - 1} \sum_{i=1}^{N_{M_L}} \left( Q_{M_L}^{(i)} - \mathbb{E}\left[Q_L\right] \right)^2.\]</div>
<p>The multilevel version of
Eq. <a class="reference internal" href="#equation-variance-est-single-level">(42)</a>
can be obtained via a telescopic expansion in term of difference of
estimators over subsequent levels. To simplify the notation and for
simplicity of exposure from now on we only indicate the level, <em>i.e.</em>
<span class="math notranslate nohighlight">\(M_\ell = \ell\)</span>.</p>
<p>The expansion is obtained by re-writing
Eq. <a class="reference internal" href="#equation-variance-est-single-level">(42)</a>
as</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{split}
\label{eq: variance_est_ML}
 \mathbb{V}ar\left[Q_L\right] &amp;\approx       \frac{1}{N_L - 1} \sum_{i=1}^{N_L} \left( Q_L^{(i)} - \mathbb{E}\left[Q_L\right] \right)^2 \\
                              &amp;\approx \sum_{\ell=0}^L  \frac{1}{N_\ell - 1} \left( \left( Q_{\ell}^{(i)} - \mathbb{E}\left[Q_{\ell}\right] \right)^2
                                                                                  - \left( Q_{{\ell-1}}^{(i)} - \mathbb{E}\left[Q_{\ell-1}\right] \right)^2 \right).
\end{split}\end{split}\]</div>
<p>It is important here to note that since the estimators at the levels
<span class="math notranslate nohighlight">\(\ell\)</span> and <span class="math notranslate nohighlight">\(\ell-1\)</span> are computed with the same number of
samples both estimators use the factor <span class="math notranslate nohighlight">\(1/(N_\ell-1)\)</span> to obtain
their unbiased version. Moreover, each estimator is indeed written with
respect to its own mean value, <em>i.e.</em> the mean value on its level,
either <span class="math notranslate nohighlight">\(\ell\)</span> or <span class="math notranslate nohighlight">\(\ell-1\)</span>. This last requirement leads to
the computation of a local expected value estimator with respect to the
same samples employed for the difference estimator. If we now denote
with <span class="math notranslate nohighlight">\(\hat{Q}_{\ell,2}\)</span> the sampling estimator for the second
order moment of the QoI <span class="math notranslate nohighlight">\(Q_\ell\)</span> we can write</p>
<div class="math notranslate nohighlight" id="equation-variance-est-ml-approximation">
<span class="eqno">(43)<a class="headerlink" href="#equation-variance-est-ml-approximation" title="Link to this equation"></a></span>\[\begin{split}
\mathbb{V}ar\left[Q_L\right] \approx \hat{Q}_{L,2}^{\mathrm{ML}} = \sum_{\ell=0}^L \hat{Q}_{\ell,2} - \hat{Q}_{\ell-1,2},
\end{split}\]</div>
<p>where</p>
<div class="math notranslate nohighlight" id="equation-variance-est-ml-level-terms">
<span class="eqno">(44)<a class="headerlink" href="#equation-variance-est-ml-level-terms" title="Link to this equation"></a></span>\[\hat{Q}_{\ell,2} = \frac{1}{N_\ell - 1} \sum_{i=1}^{N_\ell} \left( Q_\ell^{(i)} - \hat{Q}_\ell \right)^2
\quad  \mathrm{and} \quad
\hat{Q}_{\ell - 1,2} = \frac{1}{N_\ell - 1} \sum_{i=1}^{N_\ell} \left( Q_{\ell - 1}^{(i)} - \hat{Q}_{\ell - 1} \right)^2.\]</div>
<p>Note that <span class="math notranslate nohighlight">\(\hat{Q}_{\ell,2}\)</span> and <span class="math notranslate nohighlight">\(\hat{Q}_{\ell - 1,2}\)</span> are
explicitly sharing the same samples <span class="math notranslate nohighlight">\(N_\ell\)</span>.</p>
<p>For this estimator we are interested in minimizing its cost while also
prescribing its variance as done for the expected value. This is
accomplished by evaluating the variance of the multilevel variance
estimator <span class="math notranslate nohighlight">\(\hat{Q}_{L,2}^{ML}\)</span></p>
<div class="math notranslate nohighlight">
\[\mathbb{V}ar\left[ \hat{Q}_{L,2}^{\mathrm{ML}} \right] = \sum_{\ell=0}^L \mathbb{V}ar\left[ \hat{Q}_{\ell,2} - \hat{Q}_{\ell-1,2} \right]
                                               = \sum_{\ell=0}^L \mathbb{V}ar\left[ \hat{Q}_{\ell,2} \right] + \mathbb{V}ar\left[\hat{Q}_{\ell-1,2} \right]
                                               - 2 \mathbb{C}ov\left( \hat{Q}_{\ell,2},\hat{Q}_{\ell-1,2} \right),\]</div>
<p>where the covariance term is a result of the dependence described
in <a class="reference internal" href="#equation-variance-est-ml-level-terms">(44)</a>.</p>
<p>The previous expression can be evaluated once the variance for the
sample estimator of the second order order moment
<span class="math notranslate nohighlight">\(\mathbb{V}ar\left[ \hat{Q}_{\ell,2} \right]\)</span> and the covariance
term
<span class="math notranslate nohighlight">\(\mathbb{C}ov\left( \hat{Q}_{\ell,2},\hat{Q}_{\ell-1,2} \right)\)</span>
are known. These terms can be evaluated as:</p>
<div class="math notranslate nohighlight">
\[\mathbb{V}ar\left[ \hat{Q}_{\ell,2} \right] \approx \frac{1}{N_\ell} \left( \hat{Q}_{\ell,4} - \frac{N_\ell-3}{N_\ell-1} \left(\hat{Q}_{\ell,2}\right)^2 \right),\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{Q}_{\ell,4}\)</span> denotes the sampling estimator for the
fourth order central moment.</p>
<p>The expression for the covariance term is more involved and can be
written as</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{split}
 \mathbb{C}ov\left( \hat{Q}_{\ell,2},\hat{Q}_{\ell-1,2} \right) &amp;\approx \frac{1}{N_\ell} \mathbb{E}\left[ \hat{Q}_{\ell,2},\hat{Q}_{\ell-1,2} \right] \\
                                                                      &amp;+ \frac{1}{N_\ell N_{\ell-1}} \left( \mathbb{E}\left[ Q_\ell Q_{\ell-1} \right]^2
                                                                      - 2  \mathbb{E}\left[ Q_\ell Q_{\ell-1} \right] \mathbb{E}\left[ Q_\ell \right] \mathbb{E}\left[Q_{\ell-1} \right] + \left( \mathbb{E}\left[ Q_\ell \right] \mathbb{E}\left[Q_{\ell-1} \right] \right)^2
                                                                      \right).
\end{split}\end{split}\]</div>
<p>The first term of the previous expression is evaluated by estimating and
combining several sampling moments as</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{split}
 \mathbb{E}\left[ \hat{Q}_{\ell,2},\hat{Q}_{\ell-1,2} \right] &amp;= \frac{1}{N_\ell} \left( \mathbb{E}\left[ Q_\ell^2 Q_{\ell-1}^2 \right] \right) - \mathbb{E}\left[ Q_\ell^2 \right] \mathbb{E}\left[Q_{\ell-1}^2 \right] - 2 \mathbb{E}\left[Q_{\ell-1} \right] \mathbb{E}\left[ Q_{\ell}^2 Q_{\ell-1} \right] \\
                                      &amp;+ 2 \mathbb{E}\left[Q_{\ell-1}^2 \right] \mathbb{E}\left[ Q_{\ell}^2 \right]
                                      - 2  \mathbb{E}\left[ Q_{\ell} \right] \mathbb{E}\left[ Q_{\ell} Q_{\ell-1}^2 \right]
                                      + 2 \mathbb{E}\left[ Q_{\ell} \right]^2 \mathbb{E}\left[ Q_{\ell-1}^2 \right] \\
                                      &amp;+ 4 \mathbb{E}\left[ Q_{\ell} \right] \mathbb{E}\left[ Q_{\ell-1} \right] \mathbb{E}\left[ Q_{\ell} Q_{\ell-1} \right]
                                      - 4 \mathbb{E}\left[ Q_{\ell} \right]^2 \mathbb{E}\left[ Q_{\ell-1} \right]^2.
\end{split}\end{split}\]</div>
<p>It is important to note here that the previous expression can be
computed only if several sampling estimators for product of the QoIs at
levels <span class="math notranslate nohighlight">\(\ell\)</span> and <span class="math notranslate nohighlight">\(\ell-1\)</span> are available. These quantities
are not required in the standard MLMC implementation for the mean and
therefore for the estimation of the variance more data need to be stored
to assemble the quantities on each level.</p>
<p>An optimization problem, similar to the one formulated for the mean in
the previous section, can be written in the case of variance</p>
<div class="math notranslate nohighlight" id="equation-mlmc-optimization-var">
<span class="eqno">(45)<a class="headerlink" href="#equation-mlmc-optimization-var" title="Link to this equation"></a></span>\[\begin{split}
\min\limits_{N_\ell} \sum_{\ell=0}^L \mathcal{C}_{\ell} N_\ell \quad \mathrm{s.t.} \quad \mathbb{V}ar\left[ \hat{Q}_{L,2}^{\mathrm{ML}} \right] = \varepsilon^2/2.
%
%
%  f(N_\ell,\lambda) = \sum_{\ell=0}^{L} N_\ell \, \mathcal{C}_{\ell}
%                    + \lambda \left( \sum_{\ell=0}^{L} N_\ell^{-1} \mathbb{V}ar\left({Y_\ell}\right) - \varepsilon^2/2 \right).
\end{split}\]</div>
<p>This optimization problem can be solved in two different ways, namely an
analytical approximation and by solving a non-linear optimization
problem. The analytical approximation follows the approach described in
<span id="id8">[<a class="reference internal" href="../../misc/bibliography.html#id237" title="Michele Pisaroni, Sebastian Krumscheid, and Fabio Nobile. Mathicse technical report : quantifying uncertain system outputs via the multilevel monte carlo method - part 1: central moment estimation. TODO, 2017. MATHICSE Technical Report Nr. 23.2017 October 2017. URL: http://infoscience.epfl.ch/record/263564, doi:10.5075/epfl-MATHICSE-263564.">PKN17</a>]</span> and introduces a helper variable</p>
<div class="math notranslate nohighlight">
\[\hat{V}_{2, \ell} := \mathbb{V}ar\left[ \hat{Q}_{\ell,2} \right] \cdot N_{\ell}.\]</div>
<p>Next, the following constrained minimization problem is formulated</p>
<div class="math notranslate nohighlight" id="equation-mlmc-var-optimization-nobile">
<span class="eqno">(46)<a class="headerlink" href="#equation-mlmc-var-optimization-nobile" title="Link to this equation"></a></span>\[ f(N_\ell,\lambda) = \sum_{\ell=0}^{L} N_\ell \, \mathcal{C}_{\ell}
                   + \lambda \left( \sum_{\ell=0}^{L} N_\ell^{-1} \hat{V}_{2, \ell} - \varepsilon^2/2 \right),\]</div>
<p>and a closed form solution is obtained</p>
<div class="math notranslate nohighlight" id="equation-mlmc-nl-var-nobile">
<span class="eqno">(47)<a class="headerlink" href="#equation-mlmc-nl-var-nobile" title="Link to this equation"></a></span>\[N_{\ell} = \frac{2}{\varepsilon^2} \left[ \, \sum_{k=0}^L \left( \hat{V}_{2, k} \mathcal{C}_k \right)^{1/2} \right]
               \sqrt{\frac{ \hat{V}_{2, \ell} }{\mathcal{C}_{\ell}}},\]</div>
<p>similarly as for the expected value in <a class="reference internal" href="#equation-mlmc-optimization">(41)</a>.</p>
<p>The second approach uses numerical optimization directly on the
non-linear optimization
problem <a class="reference internal" href="#equation-mlmc-optimization-var">(45)</a> to
find an optimal sample allocation. Dakota uses OPTPP as the default
optimizer and switches to NPSOL if it is available.</p>
<p>Both approaches for finding the optimal sample allocation when
allocating for the variance are currently implemented in Dakota. The
analytical solution is employed by default while the optimization is
enabled using a keyword. We refer to the reference manual for a
discussion of the keywords to select these different options.</p>
</section>
<section id="mlmc-extension-to-the-standard-deviation">
<h3>MLMC extension to the standard deviation<a class="headerlink" href="#mlmc-extension-to-the-standard-deviation" title="Link to this heading"></a></h3>
<p>The extension of MLMC for the standard deviation is slightly more
complicated by the presence of the square root, which prevents a
straightforward expansion over levels.</p>
<p>One possible way of obtaining a biased estimator for the standard
deviation is</p>
<div class="math notranslate nohighlight">
\[\hat{\sigma}_L^{ML} = \sqrt{ \sum_{\ell=0}^L \hat{Q}_{\ell,2} - \hat{Q}_{\ell - 1,2} }.\]</div>
<p>To estimate the variance of the standard deviation estimator, it is
possible to leverage the result, derived in the previous section for the
variance, and write the variance of the standard deviation as a function
of the variance and its estimator variance. If we can estimate the
variance <span class="math notranslate nohighlight">\(\hat{Q}_{L,2}\)</span> and its estimator variance
<span class="math notranslate nohighlight">\(\mathbb{V}ar\left[ \hat{Q}_{L,2} \right]\)</span>, the variance for the
standard deviation <span class="math notranslate nohighlight">\(\hat{\sigma}_L^{ML}\)</span> can be approximated as</p>
<div class="math notranslate nohighlight">
\[\mathbb{V}ar\left[ \hat{\sigma}_L^{ML} \right] \approx \frac{1}{4 \hat{Q}_{L,2}} \mathbb{V}ar\left[ \hat{Q}_{L,2} \right].\]</div>
<p>Similarly to the variance case, a numerical optimization problem can be
solved to obtain the sample allocation for the estimator of the standard
deviation given a prescribed accuracy target.</p>
</section>
<section id="mlmc-extension-to-the-scalarization-function">
<h3>MLMC extension to the scalarization function<a class="headerlink" href="#mlmc-extension-to-the-scalarization-function" title="Link to this heading"></a></h3>
<p>Often, especially in the context of optimization, it is necessary to
estimate statistics of a metric defined as a linear combination of
mean and standard deviation of a QoI. A classical reliability measure
<span class="math notranslate nohighlight">\(c^{ML}[Q]\)</span> can be defined, for the quantity <span class="math notranslate nohighlight">\(Q\)</span>, starting
from multilevel (ML) statistics, as</p>
<div class="math notranslate nohighlight">
\[c_L^{ML}[Q] = \hat{Q}_{L}^{ML}  + \alpha \hat{\sigma}_L^{ML}.\]</div>
<p>To obtain the sample allocation, in the MLMC context, it is necessary
to evaluate the variance of <span class="math notranslate nohighlight">\(c_L^{ML}[Q]\)</span>, which can be written as</p>
<div class="math notranslate nohighlight">
\[\mathbb{V}ar\left[ c_L^{ML}[Q] \right] = \mathbb{V}ar\left[ \hat{Q}_{L}^{ML} \right] + \alpha^2 \mathbb{V}ar\left[ \hat{\sigma}_L^{ML} \right]
+ 2 \alpha \mathbb{C}ov\left[ \hat{Q}_{L}^{ML}, \hat{\sigma}_L^{ML} \right].\]</div>
<p>This expression requires, in addition to the already available terms
<span class="math notranslate nohighlight">\(\mathbb{V}ar\left[ \hat{Q}_{L}^{ML} \right]\)</span> and
<span class="math notranslate nohighlight">\(\mathbb{V}ar\left[ \hat{\sigma}_L^{ML} \right]\)</span>, also the
covariance term <span class="math notranslate nohighlight">\(\mathbb{C}ov\left[ \hat{Q}_{L}^{ML},
\hat{\sigma}_L^{ML} \right]\)</span>. This latter term can be written knowing
that shared samples are only present on the same level</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{split}
 \mathbb{C}ov\left[ \hat{Q}_{L}^{ML}, \hat{\sigma}_L^{ML} \right] &amp;= \mathbb{C}ov\left[ \sum_{\ell=0}^{L} \hat{Q}_{\ell} - \hat{Q}_{\ell-1}, \sum_{\ell=0}^{L} \hat{\sigma}_{\ell} - \hat{\sigma}_{\ell-1} \right] \\
                                                                  &amp;= \sum_{\ell=0}^{L} \mathbb{C}ov\left[ \hat{Q}_{\ell} - \hat{Q}_{\ell-1}, \hat{\sigma}_{\ell} - \hat{\sigma}_{\ell-1} \right],
\end{split}\end{split}\]</div>
<p>which leads to the need for evaluating the following four
contributions</p>
<div class="math notranslate nohighlight">
\[\mathbb{C}ov\left[ \hat{Q}_{\ell} - \hat{Q}_{\ell-1}, \hat{\sigma}_{\ell} - \hat{\sigma}_{\ell-1} \right] =
\mathbb{C}ov\left[ \hat{Q}_{\ell} , \hat{\sigma}_{\ell} \right] - \mathbb{C}ov\left[ \hat{Q}_{\ell} , \hat{\sigma}_{\ell-1} \right]
- \mathbb{C}ov\left[ \hat{Q}_{\ell-1}, \hat{\sigma}_{\ell} \right] + \mathbb{C}ov\left[ \hat{Q}_{\ell-1}, \hat{\sigma}_{\ell-1} \right].\]</div>
<p>In Dakota, we adopt the following approximation, for two arbitrary
levels <span class="math notranslate nohighlight">\(\ell\)</span> and
<span class="math notranslate nohighlight">\(\kappa \in \left\{ \ell-1, \ell, \ell+1 \right\}\)</span></p>
<div class="math notranslate nohighlight">
\[\rho\left[ \hat{Q}_{\ell}, \hat{\sigma}_{\kappa} \right] \approx \rho\left[ \hat{Q}_{\ell}, \hat{Q}_{\kappa,2} \right]\]</div>
<p>(we indicate with <span class="math notranslate nohighlight">\(\hat{Q}_{\kappa,2}\)</span> the second central moment
for <span class="math notranslate nohighlight">\(Q\)</span> at the level <span class="math notranslate nohighlight">\(\kappa\)</span>), which corresponds to
assuming that the correlation between expected value and variance is a
good approximation of the correlation between the expected value and
the standard deviation. This assumption is particularly convenient
because it is possible to obtain in closed form the covariance between
expected value and variance and, therefore, we can adopt the following
approximation</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{split}
 \frac{ \mathbb{C}ov\left[ \hat{Q}_{\ell}, \hat{\sigma}_{\kappa} \right]}{\sqrt{ \mathbb{V}ar\left[ \hat{Q}_{\ell} \right] \mathbb{V}ar\left[ \hat{\sigma}_{\kappa} \right]} }
 \approx \frac{\mathbb{C}ov\left[ \hat{Q}_{\ell}, \hat{Q}_{\kappa,2} \right]}{\sqrt{ \mathbb{V}ar\left[ \hat{Q}_{\ell}\right] \mathbb{V}ar\left[ \hat{Q}_{\kappa,2}\right] }} \\
 %
 \mathbb{C}ov\left[ \hat{Q}_{\ell}, \hat{\sigma}_{\kappa} \right]
 \approx \mathbb{C}ov\left[ \hat{Q}_{\ell}, \hat{Q}_{\kappa,2} \right] \frac{\sqrt{\mathbb{V}ar\left[ \hat{\sigma}_{\kappa} \right]}}{\sqrt{  \mathbb{V}ar\left[ \hat{Q}_{\kappa,2}\right] }}.
\end{split}\end{split}\]</div>
<p>Finally, we can derive the term
<span class="math notranslate nohighlight">\(\mathbb{C}ov\left[ \hat{Q}_{\ell}, \hat{Q}_{\kappa,2} \right]\)</span>
for all possible cases</p>
<div class="math notranslate nohighlight">
\[\begin{split} \mathbb{C}ov\left[ \hat{Q}_{\ell}, \hat{Q}_{\kappa,2} \right] =
 \begin{cases}
    \frac{1}{N_\ell} \left( \mathbb{E}\left[ Q_\ell Q_{\kappa}^2 \right]
                          - \mathbb{E}\left[ Q_\ell \right] \mathbb{E}\left[ Q_{\kappa}^2 \right]
                          - 2 \mathbb{E}\left[ Q_{\kappa} \right] \mathbb{E}\left[ Q_\ell Q_{\kappa} \right]
                          + 2 \mathbb{E}\left[ Q_\ell \right] \mathbb{E}\left[ Q_\kappa^2 \right]
                          \right),&amp; \text{if } \kappa \neq \ell \\
    \frac{\hat{Q}_{\ell,3}}{N_\ell},              &amp; \text{if }  \kappa = \ell.
\end{cases}\end{split}\]</div>
<p>Even for this case, the sample allocation problem can be solved by
resorting to a numerical optimization given a prescribed target.</p>
</section>
</section>
<section id="a-multilevel-multifidelity-approach">
<span id="uq-sampling-mlmf"></span><h2>A multilevel-multifidelity approach<a class="headerlink" href="#a-multilevel-multifidelity-approach" title="Link to this heading"></a></h2>
<p>The MLMC approach described in <a class="reference internal" href="#uq-sampling-multilevel"><span class="std std-ref">Multilevel Monte Carlo</span></a> can
be related to a recursive control variate technique in that it
seeks to reduce the variance of the target function in order to limit
the sampling at high resolution. In addition, the difference function
<span class="math notranslate nohighlight">\(Y_\ell\)</span> for each level can itself be the target of an additional
control variate (refer to <a class="reference internal" href="#uq-sampling-mfmc"><span class="std std-ref">Multifidelity Monte Carlo</span></a>). A
practical scenario is when not only different resolution levels are
available (multilevel part), but also a cheaper computational model can
be used (multifidelity part). The combined approach is a
multilevel-multifidelity algorithm <span id="id9">[<a class="reference internal" href="../../misc/bibliography.html#id90" title="H.R. Fairbanks, A. Doostan, C. Ketelsen, and G. Iaccarino. A low-rank control variate for multilevel monte carlo simulation of high-dimensional uncertain systems. Journal of Computational Physics, 341:121–139, 2017.">FDKI17</a>, <a class="reference internal" href="../../misc/bibliography.html#id322" title="G. Geraci, G. Iaccarino, and Michael S. Eldred. A multi fidelity control variate approach for the multilevel monte carlo technique. CTR Annual Research Briefs 2015, pages 169–181, 2015.">GIE15</a>, <a class="reference internal" href="../../misc/bibliography.html#id220" title="Fabio Nobile and Francesco Tesei. A multi level monte carlo method with control variate for elliptic pdes with log-normal coefficients. Stochastic Partial Differential Equations: Analysis and Computations, 3(3):398–444, Sep 2015.">NT15</a>]</span>, and in particular, a
multilevel-control variate Monte Carlo sampling approach.</p>
<section id="y-l-correlations">
<span id="uq-sampling-mlmf-ycorr"></span><h3><span class="math notranslate nohighlight">\(Y_l\)</span> correlations<a class="headerlink" href="#y-l-correlations" title="Link to this heading"></a></h3>
<p>If the target QoI can be generated from both a high-fidelity (HF) model
and a cheaper, possibly biased low-fidelity (LF) model, it is possible
to write the following estimator</p>
<div class="math notranslate nohighlight" id="equation-mlmf-estimator">
<span class="eqno">(48)<a class="headerlink" href="#equation-mlmf-estimator" title="Link to this equation"></a></span>\[\mathbb{E}\left[Q_M^{\mathrm{HF}}\right] = \sum_{l=0}^{L_{\mathrm{HF}}} \mathbb{E}\left[Y^{\mathrm{HF}}_{\ell}\right]
                                          \approx \sum_{l=0}^{L_{\mathrm{HF}}} \hat{Y}^{\mathrm{HF}}_{\ell} = \sum_{l=0}^{L_{\mathrm{HF}}} Y^{{\mathrm{HF}},\star}_{\ell},\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[Y^{{\mathrm{HF}},\star}_{\ell} = Y^{\mathrm{HF}}_{\ell} + \alpha_\ell \left( \hat{Y}^{\mathrm{LF}}_{\ell} - \mathbb{E}\left[{Y^{\mathrm{LF}}_{\ell}}\right] \right).\]</div>
<p>The estimator <span class="math notranslate nohighlight">\(Y^{\mathrm{HF},\star}_{\ell}\)</span> is unbiased with
respect to <span class="math notranslate nohighlight">\(\hat{Y}^{\mathrm{HF}}_{\ell}\)</span>, hence with respect to
the true value <span class="math notranslate nohighlight">\(\mathbb{E}\left[Y^{\mathrm{HF}}_{\ell}\right]\)</span>.
The control variate is obtained by means of the LF model realizations
for which the expected value can be computed in two different ways:
<span class="math notranslate nohighlight">\(\hat{Y}^{\mathrm{LF}}_{\ell}\)</span> and
<span class="math notranslate nohighlight">\(\mathbb{E}\left[Y^{\mathrm{LF}}_{\ell}\right]\)</span>. A MC estimator is
employed for each term but the estimation of
<span class="math notranslate nohighlight">\(\mathbb{E}\left[Y^{\mathrm{LF}}_{\ell}\right]\)</span> is more resolved
than <span class="math notranslate nohighlight">\(\hat{Y}^{\mathrm{LF}}_{\ell}\)</span>. For
<span class="math notranslate nohighlight">\(\hat{Y}^{\mathrm{LF}}_{\ell}\)</span>, we choose the number of LF
realizations to be equal to the number of HF realizations,
<span class="math notranslate nohighlight">\(N_{\ell}^{\mathrm{HF}}\)</span>. For the more resolved
<span class="math notranslate nohighlight">\(\mathbb{E}\left[Y^{\mathrm{LF}}_{\ell}\right]\)</span>, we augment with
an additional and independent set of realizations
<span class="math notranslate nohighlight">\(\Delta_{\ell}^{\mathrm{LF}}\)</span>, hence
<span class="math notranslate nohighlight">\(N_{\ell}^{\mathrm{LF}} = N_{\ell}^{\mathrm{HF}} + \Delta_{\ell}^{\mathrm{LF}}\)</span>.
The set <span class="math notranslate nohighlight">\(\Delta_{\ell}^{\mathrm{LF}}\)</span> is written, for convenience,
as proportional to <span class="math notranslate nohighlight">\(N_{\ell}^{\mathrm{HF}}\)</span> by means of a
parameter <span class="math notranslate nohighlight">\(r_{\ell} \in \mathbb{R}^+_0\)</span></p>
<div class="math notranslate nohighlight">
\[N_{\ell}^{\mathrm{LF}} = N_{\ell}^{\mathrm{HF}} + \Delta_{\ell}^{\mathrm{LF}} = N_{\ell}^{\mathrm{HF}} + r_{\ell} N_{\ell}^{\mathrm{HF}}
                        = N_{\ell}^{\mathrm{HF}} (1 + r_{\ell}).\]</div>
<p>The set of samples <span class="math notranslate nohighlight">\(\Delta_{\ell}^{\mathrm{LF}}\)</span> is independent of
<span class="math notranslate nohighlight">\(N_{\ell}^{\mathrm{HF}}\)</span>, therefore the variance of the estimator
can be written as (for further details see
<span id="id10">[<a class="reference internal" href="../../misc/bibliography.html#id322" title="G. Geraci, G. Iaccarino, and Michael S. Eldred. A multi fidelity control variate approach for the multilevel monte carlo technique. CTR Annual Research Briefs 2015, pages 169–181, 2015.">GIE15</a>]</span>)</p>
<div class="math notranslate nohighlight" id="equation-mlmf-mean">
<span class="eqno">(49)<a class="headerlink" href="#equation-mlmf-mean" title="Link to this equation"></a></span>\[\begin{split}\begin{split}
\mathbb{V}ar\left(\hat{Q}_M^{MLMF}\right) &amp;= \sum_{l=0}^{L_{\mathrm{HF}}} \left( \dfrac{1}{N_{\ell}^{\mathrm{HF}}} \mathbb{V}ar\left(Y^{\mathrm{HF}}_{\ell}\right)
                                          + \dfrac{\alpha_\ell^2 r_\ell}{(1+r_\ell) N_{\ell}^{\mathrm{HF}}} \mathbb{V}ar\left(Y^{\mathrm{HF}}_{\ell}\right) \right. \\
              &amp;+  \left. 2 \dfrac{\alpha_\ell r_\ell^2}{(1+r_\ell) N_{\ell}^{\mathrm{HF}}} \rho_\ell \sqrt{ \mathbb{V}ar\left(Y^{\mathrm{HF}}_{\ell}\right)
                                                                                                      \mathbb{V}ar\left(Y^{\mathrm{LF}}_{\ell}\right) } \right),
\end{split}\end{split}\]</div>
<p>The Pearson’s correlation coefficient between the HF and LF models is
indicated by <span class="math notranslate nohighlight">\(\rho_\ell\)</span> in the previous equations. Assuming the
vector <span class="math notranslate nohighlight">\(r_\ell\)</span> as a parameter, the variance is minimized per
level, mimicking the standard control variate approach, and thus
obtaining the optimal coefficient as
<span class="math notranslate nohighlight">\(\alpha_\ell = -\rho_\ell \sqrt{ \dfrac{ \mathbb{V}ar\left( Y^{\mathrm{HF}}_{\ell} \right) }{ \mathbb{V}ar\left( Y^{\mathrm{LF}}_{\ell}  \right)     }}\)</span>.
By making use of the optimal coefficient <span class="math notranslate nohighlight">\(\alpha_\ell\)</span>, it is
possible to show that the variance
<span class="math notranslate nohighlight">\(\mathbb{V}ar\left(Y^{\mathrm{HF},\star}_{\ell}\right)\)</span> is
proportional to the variance
<span class="math notranslate nohighlight">\(\mathbb{V}ar\left(Y^{\mathrm{HF}}_{\ell}\right)\)</span> through a factor
<span class="math notranslate nohighlight">\(\Lambda_{\ell}(r_\ell)\)</span>, which is an explicit function of the
ratio <span class="math notranslate nohighlight">\(r_\ell\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-mlmf-variance">
<span class="eqno">(50)<a class="headerlink" href="#equation-mlmf-variance" title="Link to this equation"></a></span>\[\begin{split}\begin{split}
 \mathbb{V}ar\left(\hat{Q}_M^{MLMF}\right) &amp;= \sum_{l=0}^{L_{\mathrm{HF}}} \dfrac{1}{N_{\ell}^{\mathrm{HF}}} \mathbb{V}ar\left(Y^{\mathrm{HF}}_{\ell}\right)
 \Lambda_{\ell}(r_\ell) \quad \mathrm{where} \\
 \Lambda_{\ell}(r_\ell) &amp;= \left( 1 - \dfrac{r_\ell}{1+r_\ell}\rho_\ell^2 \right).
\end{split}\end{split}\]</div>
<p>Note that <span class="math notranslate nohighlight">\(\Lambda_{\ell}(r_\ell)\)</span> represents a penalty with
respect to the classical control variate approach presented in <a class="reference internal" href="#uq-sampling-mfmc"><span class="std std-ref">Multifidelity Monte Carlo</span></a>, which stems from the need to
evaluate the unknown function
<span class="math notranslate nohighlight">\(\mathbb{E}\left[Y^{\mathrm{LF}}_{\ell}\right]\)</span>. However, the
ratio <span class="math notranslate nohighlight">\(r_\ell/(r_\ell+1)\)</span> is dependent on the additional number of
LF evaluations <span class="math notranslate nohighlight">\(\Delta_{\ell}^{\mathrm{LF}}\)</span>, hence it is fair to
assume that it can be made very close to unity by choosing an affordably
large <span class="math notranslate nohighlight">\(r_\ell\)</span>, i.e.,
<span class="math notranslate nohighlight">\(\Delta_{\ell}^{\mathrm{LF}} &gt;&gt; N_{\ell}^{\mathrm{HF}}\)</span>.</p>
<p>The optimal sample allocation is determined taking into account the
relative cost between the HF and LF models and their correlation (per
level). In particular the optimization problem introduced in
Eq. <a class="reference internal" href="#equation-mlmc-optimization">(41)</a> is replaced by</p>
<div class="math notranslate nohighlight">
\[\mathrm{argmin}_{N_{\ell}^{\mathrm{HF}}, r_\ell}(\mathcal{L}), \quad \mathrm{where} \quad \mathcal{L} = \sum_{\ell=0}^{L_{\mathrm{HF}}} N_{\ell}^{\mathrm{HF}} \mathcal{C}_{\ell}^{\mathrm{eq}} +
                 \lambda \left( \sum_{\ell=0}^{L_{\mathrm{HF}}} \dfrac{1}{N_{\ell}^{\mathrm{HF}}}\mathbb{V}ar\left( Y^{\mathrm{HF}}_{\ell}\right) \Lambda_{\ell}(r_\ell) - \varepsilon^2/2 \right),\]</div>
<p>where the optimal allocation is obtained as well as the optimal ratio
<span class="math notranslate nohighlight">\(r_\ell\)</span>. The cost per level includes now the sum of the HF and LF
realization cost, therefore it can be expressed as
<span class="math notranslate nohighlight">\(\mathcal{C}_{\ell}^{\mathrm{eq}} = \mathcal{C}_{\ell}^{\mathrm{HF}} + \mathcal{C}_{\ell}^{\mathrm{LF}} (1+r_\ell)\)</span>.</p>
<p>If the cost ratio between the HF and LF model is
<span class="math notranslate nohighlight">\(w_{\ell} =  \mathcal{C}_{\ell}^{\mathrm{HF}} / \mathcal{C}_{\ell}^{\mathrm{LF}}\)</span>
then the optimal ratio is</p>
<div class="math notranslate nohighlight">
\[r_\ell^{\star} = -1 + \sqrt{ \dfrac{\rho_\ell^2}{1-\rho_\ell^2} w_{\ell}},\]</div>
<p>and the optimal allocation is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  N_{\ell}^{\mathrm{HF},\star} &amp;= \frac{2}{\varepsilon^2} \!\! \left[ \, \sum_{k=0}^{L_{\mathrm{HF}}}
        \left( \dfrac{ \mathbb{V}ar\left(  Y_k^{ \mathrm{HF} } \right) \mathcal{C}_{k}^{\mathrm{HF}}}{1-\rho_\ell^2} \right)^{1/2} \Lambda_{k}(r_k^{\star}) \right]
               \sqrt{ \left( 1 - \rho_\ell^2 \right) \frac{ \mathbb{V}ar\left(Y^{\mathrm{HF}}_{\ell}\right) }{\mathcal{C}_{\ell}^{\mathrm{HF}}}}.
\end{split}\]</div>
<p>It is clear that the efficiency of the algorithm is related not only to
the efficiency of the LF model, i.e. how fast a simulation runs with
respect to the HF model, but also to the correlation between the LF and
HF model.</p>
</section>
<section id="q-l-correlations">
<span id="uq-sampling-mlmf-qcorr"></span><h3><span class="math notranslate nohighlight">\(Q_l\)</span> correlations<a class="headerlink" href="#q-l-correlations" title="Link to this heading"></a></h3>
<p>A potential refinement of the previous approach <span id="id11">[<a class="reference internal" href="../../misc/bibliography.html#id105" title="Gianluca Geraci, Michael S. Eldred, and Gianluca Iaccarino. A multifidelity multilevel Monte Carlo method for uncertainty propagation in aerospace applications. In 19th AIAA Non-Deterministic Approaches Conference. AIAA, January 2017. URL: http://arc.aiaa.org/doi/10.2514/6.2017-1951 (visited on 2019-10-04), doi:10.2514/6.2017-1951.">GEI17</a>]</span> consists in exploiting
the QoI on each pair of levels, <span class="math notranslate nohighlight">\(\ell\)</span> and <span class="math notranslate nohighlight">\(\ell-1\)</span>, to
build a more correlated LF function. For instance, it is possible to use</p>
<div class="math notranslate nohighlight">
\[\mathring{Y}^{\mathrm{LF}}_{\ell} =  \gamma_\ell Q_\ell^{\mathrm{LF}} - Q_{\ell-1}^{\mathrm{LF}}\]</div>
<p>and maximize the correlation between <span class="math notranslate nohighlight">\(Y_\ell^{\mathrm{HF}}\)</span> and
<span class="math notranslate nohighlight">\(\mathring{Y}^{\mathrm{LF}}_{\ell}\)</span> through the coefficient
<span class="math notranslate nohighlight">\(\gamma_\ell\)</span>.</p>
<p>Formally the two formulations are completely equivalent if
<span class="math notranslate nohighlight">\(Y_\ell^{\mathrm{LF}}\)</span> is replaced with
<span class="math notranslate nohighlight">\(\mathring{Y}^{\mathrm{LF}}_{\ell}\)</span> in
Eq. <a class="reference internal" href="#equation-mlmf-estimator">(48)</a> and they can be
linked through the two ratios</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{split}
 \theta_{\ell} &amp;= \dfrac{  \mathrm{Cov}\left(  Y^{\mathrm{HF}}_{\ell},\mathring{Y}^{\mathrm{LF}}_{\ell} \right)   }
                        {  \mathrm{Cov}\left( Y^{\mathrm{HF}}_{\ell},Y^{\mathrm{LF}}_{\ell} \right)  } \\
 \quad \tau_{\ell}  &amp;= \dfrac{  \mathbb{V}ar\left(  \mathring{Y}^{\mathrm{LF}}_{\ell} \right)  }{ \mathbb{V}ar\left( Y^{\mathrm{LF}}_{\ell} \right) },
 \end{split}\end{split}\]</div>
<p>obtaining the following variance for the estimator</p>
<div class="math notranslate nohighlight">
\[\mathbb{V}ar\left(\hat{Q}_M^{MLMF} \right) = \dfrac{1}{N_{\ell}^{\mathrm{HF}}} \mathbb{V}ar\left( Y^{\mathrm{HF}}_{\ell} \right)
 \left( 1 - \dfrac{r_\ell}{1+r_\ell} \rho_\ell^2 \dfrac{\theta_\ell^2}{\tau_\ell} \right).\]</div>
<p>Therefore, a way to increase the variance reduction is to maximize the
ratio <span class="math notranslate nohighlight">\(\dfrac{\theta_\ell^2}{\tau_\ell}\)</span> with respect to the
parameter <span class="math notranslate nohighlight">\(\gamma_\ell\)</span>. It is possible to solve analytically this
maximization problem obtaining</p>
<div class="math notranslate nohighlight">
\[\gamma_\ell^\star= \dfrac{ \mathrm{Cov}\left(  Y^{\mathrm{HF}}_{\ell},Q_{\ell-1}^{\mathrm{LF}} \right) \mathrm{Cov}\left( Q_{\ell}^{\mathrm{LF}},Q_{\ell-1}^{\mathrm{LF}} \right)
                   - \mathbb{V}ar\left(Q_{\ell-1}^{\mathrm{LF}}\right) \mathrm{Cov}\left(  Y^{\mathrm{HF}}_{\ell},Q_{\ell}^{\mathrm{LF}} \right) }
            { \mathbb{V}ar\left(Q_{\ell}^{\mathrm{LF}}\right) \mathrm{Cov}\left( Y^{\mathrm{HF}}_{\ell},Q_{\ell-1}^{\mathrm{LF}} \right)
            - \mathrm{Cov}\left( Y^{\mathrm{HF}}_{\ell},Q_{\ell}^{\mathrm{LF}} \right) \mathrm{Cov}\left( Q_{\ell}^{\mathrm{LF}},Q_{\ell-1}^{\mathrm{LF}} \right) }.\]</div>
<p>The resulting optimal allocation of samples across levels and model
forms is given by</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{split}
  r_\ell^{\star} &amp;= -1 + \sqrt{ \dfrac{\rho_l^2 \dfrac{\theta_\ell^2}{\tau_\ell} }{1-\rho_\ell^2 \dfrac{\theta_\ell^2}{\tau_\ell}} w_{\ell}}, \quad \mathrm{where} \quad w_{\ell}
               =  \mathcal{C}_{\ell}^{\mathrm{HF}} / \mathcal{C}_{\ell}^{\mathrm{LF}}\\
  \Lambda_{\ell} &amp;= 1 - \rho_\ell^2 \dfrac{\theta_\ell^2}{\tau_\ell} \dfrac{r_\ell^{\star}}{1+r_\ell^{\star}}\\
  N_{\ell}^{\mathrm{HF},\star} &amp;= \frac{2}{\varepsilon^2} \!\! \left[ \, \sum_{k=0}^{ L_{\mathrm{HF}} }
       \left( \dfrac{ \mathbb{V}ar\left(Y_k^{ \mathrm{HF} } \right) \mathcal{C}_{k}^{\mathrm{HF}}}{1-\rho_\ell^2 \dfrac{\theta_\ell^2}{\tau_\ell}} \right)^{1/2} \Lambda_{k}(r_k^{\star})\right]
               \sqrt{ \left( 1 - \rho_\ell^2 \dfrac{\theta_\ell^2}{\tau_\ell} \right) \frac{ \mathbb{V}ar\left( Y^{\mathrm{HF}}_{\ell} \right) }{\mathcal{C}_{\ell}^{\mathrm{HF}}}}
 \end{split}\end{split}\]</div>
</section>
</section>
<section id="quasi-monte-carlo-qmc">
<span id="uq-sampling-quasimontecarlo"></span><h2>Quasi-Monte Carlo (QMC)<a class="headerlink" href="#quasi-monte-carlo-qmc" title="Link to this heading"></a></h2>
<p>Quasi-Monte Carlo methods are equal-weight quadrature rules to approximate
<span class="math notranslate nohighlight">\(\mathbb{E}\left[Q\right]\)</span> with deterministically well-chosen sample
points to beat the notoriously slow convergence of a method that uses random MC
samples. They are of the form</p>
<div class="math notranslate nohighlight" id="equation-qmc">
<span class="eqno">(51)<a class="headerlink" href="#equation-qmc" title="Link to this equation"></a></span>\[\hat{Q}_N^{QMC} = \dfrac{1}{N} \sum_{i=1}^N Q(\boldsymbol{t}^{(i)}),\]</div>
<p>which is seemingly identical to the form of the classic MC method from Eq.
<a class="reference internal" href="#equation-mc">(37)</a>, however, the <span class="math notranslate nohighlight">\(N\)</span> <span class="math notranslate nohighlight">\(s\)</span>-dimensional points
<span class="math notranslate nohighlight">\(\boldsymbol{t}^{(i)}\)</span> are now carefully chosen inside the domain
<span class="math notranslate nohighlight">\(\Xi \subset \mathbb{R}^d\)</span>. With <em>carefully chosen</em> we mean that the
points have a low discrepancy <span class="math notranslate nohighlight">\(D(\boldsymbol{t}^{(0)},
\boldsymbol{t}^{(1)}, \ldots, \boldsymbol{t}^{(N-1)})\)</span>. This discrepancy is
important, because it directly appears in the error bound of a QMC method, i.e.,
we have the Koksma-Hlawka inequality <span id="id12">[<a class="reference internal" href="../../misc/bibliography.html#id210" title="Harald Niederreiter. Random number generation and quasi-Monte Carlo methods. SIAM, 1992.">Nie92</a>]</span></p>
<div class="math notranslate nohighlight" id="equation-koksmahlawka">
<span class="eqno">(52)<a class="headerlink" href="#equation-koksmahlawka" title="Link to this equation"></a></span>\[|\mathbb{E}\left[Q\right] - \hat{Q}_N^{QMC}| \leq D(\boldsymbol{t}^{(0)},
\boldsymbol{t}^{(1)}, \ldots, \boldsymbol{t}^{(N-1)}) V(f).\]</div>
<p>The QMC error thus consists of two parts: a factor that only depends on the
point set (in particular, on the discrepancy of the point set) and a factor
depending only on the function <span class="math notranslate nohighlight">\(f\)</span> we are trying to integrate (the
so-called variation of the function <span class="math notranslate nohighlight">\(f`\)</span>).</p>
<p>Some famous examples of low-discrepancy point sets are Sobol points
<span id="id13">[<a class="reference internal" href="../../misc/bibliography.html#id271" title="Il'ya Meerovich Sobol'. On the distribution of points in a cube and the approximate evaluation of integrals. Zhurnal Vychislitel'noi Matematiki i Matematicheskoi Fiziki, 7(4):784–802, 1967.">Sobol67</a>]</span>, Halton points <span id="id14">[<a class="reference internal" href="../../misc/bibliography.html#id141" title="J. H. Halton and G. B. Smith. Algorithm 247: radical-inverse quasi-random point sequence. Communications of the ACM, 7:701–702, 1964.">HS64</a>]</span> and Hammersley points
<span id="id15">[<a class="reference internal" href="../../misc/bibliography.html#id142" title="John Hammersley. Monte carlo methods. Springer Science &amp; Business Media, 2013.">Ham13</a>]</span>. The advantage of using such a low-discrepancy point set
is faster convergence: classic theory states that a QMC method may converge like
<span class="math notranslate nohighlight">\((\log N)^d/N\)</span>, for sufficiently smooth functions <span class="math notranslate nohighlight">\(f\)</span>, see
<span id="id16">[<a class="reference internal" href="../../misc/bibliography.html#id56" title="Josef Dick and Friedrich Pillichshammer. Digital nets and sequences: discrepancy theory and quasi–Monte Carlo integration. Cambridge University Press, 2010.">DP10</a>]</span>. Compare this to the classic MC method, that converges like
<span class="math notranslate nohighlight">\(1/\sqrt{N}\)</span>, and it is easy to see why QMC methods are so appealing.</p>
<p>Unfortunately, the classic QMC theory is not adequate in high dimensions (large
<span class="math notranslate nohighlight">\(d\)</span>): for <span class="math notranslate nohighlight">\((\log N)^d/N\)</span> to be smaller than <span class="math notranslate nohighlight">\(1/\sqrt{N}\)</span>, we
require, for example, <span class="math notranslate nohighlight">\(N &gt; \exp(d)\)</span>, an unrealistically large number in
high dimensions. Furthermore, in many problems, the variation <span class="math notranslate nohighlight">\(V(f)\)</span> is
infinite, making the error bound in <a class="reference internal" href="#equation-koksmahlawka">(52)</a> practically
useless.</p>
<p>Then, in 1995, a 360-dimensional integral originating from financial mathematics
was computed very efficiently by Paskov and Traub, see <span id="id17">[<a class="reference internal" href="../../misc/bibliography.html#id230" title="Spassimir Paskov and Joseph F Traub. Faster valuation of financial derivatives. Journal of Portfolio Management, 1996.">PT96</a>]</span>. This
led to many new theoretical developments, including the notion of <em>weighted</em>
function spaces and <em>low effective dimension</em>: although the problem is
high-dimensional, not all dimensions are equally important. In the work by Sloan
and Woźniakowski <span id="id18">[<a class="reference internal" href="../../misc/bibliography.html#id267" title="Ian H Sloan and Henryk Woźniakowski. When are quasi-monte carlo algorithms efficient for high dimensional integrals? Journal of Complexity, 14(1):1–33, 1998.">SWozniakowski98</a>]</span>, this decreasing importance is quantified in
terms of weights <span class="math notranslate nohighlight">\(\gamma_j\)</span> associated to each dimension <span class="math notranslate nohighlight">\(j\)</span>, and
where one assumes <span class="math notranslate nohighlight">\(\gamma_1 \geq \gamma_2 \geq \ldots \geq \gamma_d \geq
0\)</span>. Contemporary QMC analysis is then performed by analyzing the problem in a
function space that incorporates these weights. A reinterpretation of
<a class="reference internal" href="#equation-koksmahlawka">(52)</a> in the weighted space setting with weights
<span class="math notranslate nohighlight">\(\boldsymbol{\gamma}\)</span> is then</p>
<div class="math notranslate nohighlight">
\[|\mathbb{E}\left[Q\right] - \hat{Q}_N^{QMC}| \leq e_{\boldsymbol{\gamma}}(\boldsymbol{t}^{(0)},
\boldsymbol{t}^{(1)}, \ldots, \boldsymbol{t}^{(N-1)}) \|f\|_{\boldsymbol{\gamma}},\]</div>
<p>where <span class="math notranslate nohighlight">\(e_{\boldsymbol{\gamma}}\)</span> is the so-called <em>worst-case error</em>, and
<span class="math notranslate nohighlight">\(\|f\|_{\boldsymbol{\gamma}}\)</span> is the norm of the function in the weighted
function space. The question then becomes one of (strong) <em>tractability</em>: under
which conditions on the weights is the worst-case error bounded independent of
the dimension <span class="math notranslate nohighlight">\(d\)</span>? The philosophy of modern QMC is therefore to choose the
weights according to the problem at hand, and then construct a QMC method that
yields good performance for all functions that belong to this weighted function
space, see <span id="id19">[<a class="reference internal" href="../../misc/bibliography.html#id56" title="Josef Dick and Friedrich Pillichshammer. Digital nets and sequences: discrepancy theory and quasi–Monte Carlo integration. Cambridge University Press, 2010.">DP10</a>]</span>.</p>
<p>QMC methods come in two major flavors: <em>lattice rules</em> and <em>digital nets</em>. We
will now briefly discuss these two construction methods.</p>
<section id="rank-1-lattice-rules-and-sequences">
<h3>Rank-1 lattice rules and sequences<a class="headerlink" href="#rank-1-lattice-rules-and-sequences" title="Link to this heading"></a></h3>
<p>An <span class="math notranslate nohighlight">\(N\)</span>-point rank-1 lattice rule in <span class="math notranslate nohighlight">\(d\)</span> dimensions generates points
according to</p>
<div class="math notranslate nohighlight" id="equation-rank1lattice">
<span class="eqno">(53)<a class="headerlink" href="#equation-rank1lattice" title="Link to this equation"></a></span>\[\boldsymbol{t}^{(i)} = \left\{ \frac{i \boldsymbol{z}}{N} \right\} = \frac{i \boldsymbol{z} \;\text{mod}\; N}{N}\]</div>
<p>where <span class="math notranslate nohighlight">\(\{\;\cdot\;\}\)</span> denotes the fractional part, i.e., <span class="math notranslate nohighlight">\(\{x\} = x
- \lceil x \rceil\)</span>, and where <span class="math notranslate nohighlight">\(\boldsymbol{z} = (z_1, z_2, \ldots, z_d)\)</span>
is an <span class="math notranslate nohighlight">\(d\)</span>-dimensional vector with integers, called the <em>generating
vector</em>. Rank-1 lattices were introduced by Korobov <span id="id20">[<a class="reference internal" href="../../misc/bibliography.html#id174" title="AN Korobov. The approximate computation of multiple integrals. In Dokl. Akad. Nauk SSSR, volume 124, 1207–1210. 1959.">Kor59</a>]</span> and
Hlawka <span id="id21">[<a class="reference internal" href="../../misc/bibliography.html#id153" title="Edmund Hlawka. Zur angenäherten berechnung mehrfacher integrale. Monatshefte für Mathematik, 66(2):140–151, 1962.">Hla62</a>]</span>, as the <em>method of good lattice points</em>.</p>
<p>The performance of the lattice rule depends critically on the choice of the
generating vector <span class="math notranslate nohighlight">\(\boldsymbol{z}\)</span>. We assume that <span class="math notranslate nohighlight">\(z \in
\mathbb{U}_N^d\)</span>, where <span class="math notranslate nohighlight">\(\mathbb{U}_N = \{ z \in \mathbb{Z} : 1 \leq z \leq
N - 1 \;\text{and}\; \mathrm{gcd}(z, N) = 1\}\)</span>, to ensure that every
one-dimensional projection of the <span class="math notranslate nohighlight">\(N\)</span> points on one of the coordinate axes
has <span class="math notranslate nohighlight">\(N\)</span> distinct values. It can be shown that the number of elements
inside the set <span class="math notranslate nohighlight">\(\mathbb{U}_N\)</span> is given by the Euler totient function
<span class="math notranslate nohighlight">\(\varphi(N)\)</span>. For number theoretical reasons, <span class="math notranslate nohighlight">\(N\)</span> is usually
restricted to be a prime number, such that the number of elements is
<span class="math notranslate nohighlight">\(\varphi(N) = N-1\)</span>. In that case, there are an astounding <span class="math notranslate nohighlight">\((N -
1)^d\)</span> possible choices for the generating vector <span class="math notranslate nohighlight">\(\boldsymbol{z}\)</span>. Since
it is impossible to perform an exhaustive search over all possible choices for
large <span class="math notranslate nohighlight">\(N\)</span> and <span class="math notranslate nohighlight">\(s\)</span> to find the best possible generating vector
<span class="math notranslate nohighlight">\(\boldsymbol{z}\)</span>, we resort to construction schemes that deliver good
choices for <span class="math notranslate nohighlight">\(\boldsymbol{z}\)</span>. An example of such a scheme is the
component-by-component (CBC) construction <span id="id22">[<a class="reference internal" href="../../misc/bibliography.html#id175" title="Nikolai Mikhailovich Korobov. Number-theoretic methods in approximate analysis. 1963.">Kor63</a>, <a class="reference internal" href="../../misc/bibliography.html#id266" title="Ian H Sloan and Stephen Joe. Lattice methods for multiple integration. Oxford University Press, 1994.">SJ94</a>]</span>. The
algorithm works as follows:</p>
<ol class="arabic simple">
<li><p>Set <span class="math notranslate nohighlight">\(z_1=1\)</span>.</p></li>
<li><p>With <span class="math notranslate nohighlight">\(z_1\)</span> fixed, pick <span class="math notranslate nohighlight">\(z_2 \in \mathbb{U}_N\)</span> such that
<span class="math notranslate nohighlight">\(e_{\boldsymbol{\gamma}}(z_1, z_2)\)</span> is minimized.</p></li>
<li><p>With <span class="math notranslate nohighlight">\(z_1\)</span> and <span class="math notranslate nohighlight">\(z_2\)</span> fixed, pick <span class="math notranslate nohighlight">\(z_3 \in \mathbb{U}_N\)</span>
such that <span class="math notranslate nohighlight">\(e_{\boldsymbol{\gamma}}(z_1, z_2, z_3)\)</span> is minimized.</p></li>
<li><p>…</p></li>
</ol>
<p>Hence, this algorithm constructs the components of the generating vector for the
lattice rule one at a time: the <span class="math notranslate nohighlight">\((j + 1)\)</span>th component is obtained by
successive one-dimensional searches, with the previous <span class="math notranslate nohighlight">\(j\)</span> components kept
fixed. It can be proven that the CBC algorithm constructs generating vectors
that, when used in a lattice rule, achieve the desired convergence close to
<span class="math notranslate nohighlight">\(1/N\)</span>, in some weighted function space, see <span id="id23">[<a class="reference internal" href="../../misc/bibliography.html#id178" title="Frances Y Kuo. Component-by-component constructions achieve the optimal rate of convergence for multivariate integration in weighted korobov and sobolev spaces. Journal of Complexity, 19(3):301–320, 2003.">Kuo03</a>]</span>.</p>
<p>For some particular choices of the weights <span class="math notranslate nohighlight">\(\boldsymbol{\gamma}\)</span> (called
product weights), the cost of the CBC algorithm is <span class="math notranslate nohighlight">\(O(d N \log N )\)</span>
operations, i.e., linear in the dimension <span class="math notranslate nohighlight">\(d\)</span> and almost linear in the
number of points <span class="math notranslate nohighlight">\(N\)</span>, due to a fast CBC construction by Nuyens and Cools,
see <span id="id24">[<a class="reference internal" href="../../misc/bibliography.html#id43" title="Ronald Cools, Frances Y Kuo, and Dirk Nuyens. Constructing embedded lattice rules for multivariate integration. SIAM Journal on Scientific Computing, 28(6):2162–2188, 2006.">CKN06</a>, <a class="reference internal" href="../../misc/bibliography.html#id212" title="Dirk Nuyens and Ronald Cools. Fast algorithms for component-by-component construction of rank-1 lattice rules in shift-invariant reproducing kernel Hilbert spaces. Mathematics of Computation, 75(254):903–920, 2006.">NC06</a>]</span>. The idea for the fast construction is that the
CBC construction requires the evaluation of a matrix-vector multiplication with
a circulant matrix, hence reducing the cost of the matrix-vector product from
<span class="math notranslate nohighlight">\(O(N^2)\)</span> to <span class="math notranslate nohighlight">\(O(N \log N)\)</span> by using FFT.</p>
<figure class="align-center" id="sampling-randomshift">
<img alt="" src="../../_images/random_shift.png" />
<figcaption>
<p><span class="caption-number">Fig. 64 </span><span class="caption-text">Applying a <span class="math notranslate nohighlight">\((1/10, 1/3)\)</span>-shift to a 21-point Fibonacci lattice in two
dimensions. Take the original lattice (<em>left</em>), apply a random shift
(<em>middle</em>) and wrap the points back onto the unit square (<em>right</em>).</span><a class="headerlink" href="#sampling-randomshift" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>The lattice points given in <a class="reference internal" href="#equation-rank1lattice">(53)</a> can be randomized by
adding a <em>random shift</em> vector. If <span class="math notranslate nohighlight">\(\Delta\)</span> is a <span class="math notranslate nohighlight">\(d\)</span>-dimensional
vector of standard normal random variables, we construct the shifted lattice
points as</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{t}_n = \left\{ \frac{n \boldsymbol{z}}{N} + \Delta \right\}.\]</div>
<p>This procedure is illustrated in  <a class="reference internal" href="#sampling-randomshift"><span class="std std-numref">Fig. 64</span></a>. Note that the
first untransformed point in the sequence will be
<span class="math notranslate nohighlight">\(\boldsymbol{t}^{(0)} = (0, 0, \ldots, 0)\)</span>.</p>
<p>For the lattice points to be practically useful, we would like to transform the
lattice rule into a <em>lattice sequence</em>, that allows us to generate
well-distributed points for an arbitrary number of points <span class="math notranslate nohighlight">\(N\)</span>. To this
end, Eq. <a class="reference internal" href="#equation-rank1lattice">(53)</a> is adapted to</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{t}^{(i)} = \left\{ \phi_b(i) \boldsymbol{z} \right\},\]</div>
<p>where <span class="math notranslate nohighlight">\(\phi_b(i)\)</span> denotes the so-called <em>radical inverse</em> function in base
<span class="math notranslate nohighlight">\(b\)</span> (usually, <span class="math notranslate nohighlight">\(b = 2\)</span>). This function transforms a number <span class="math notranslate nohighlight">\(i =
(\ldots i_2i_1)_b\)</span> in its base-<span class="math notranslate nohighlight">\(b\)</span> representation to <span class="math notranslate nohighlight">\(\phi_b(i) =
(0.i_1i_2\ldots)_b\)</span>. Note that the radical inverse function agrees with the
original formulation when <span class="math notranslate nohighlight">\(N = b^m\)</span> for any <span class="math notranslate nohighlight">\(m \geq 0\)</span>.</p>
</section>
<section id="digital-nets-and-sequences">
<h3>Digital nets and sequences<a class="headerlink" href="#digital-nets-and-sequences" title="Link to this heading"></a></h3>
<p>Digital nets and sequences were introduced by Niederreiter, building upon
earlier work by Sobol and Faure <span id="id25">[<a class="reference internal" href="../../misc/bibliography.html#id218" title="Harald Niederreiter. Point sets and sequences with small discrepancy. Monatshefte für Mathematik, 104:273–337, 1987.">Nie87</a>]</span>. In the digital
construction scheme, a sequence in <span class="math notranslate nohighlight">\(d\)</span> dimensions generates points
<span class="math notranslate nohighlight">\(\boldsymbol{t}^{(i)} = (t_{i, 0}, t_{i, 1}, \ldots, t_{i, d})\)</span>, where the
<span class="math notranslate nohighlight">\(j\)</span>th component <span class="math notranslate nohighlight">\(t_{i, j}\)</span> is constructed as follows:</p>
<ol class="arabic simple">
<li><p>Write <span class="math notranslate nohighlight">\(i\)</span> in its base-<span class="math notranslate nohighlight">\(b\)</span> representation, i.e.,</p></li>
</ol>
<div class="math notranslate nohighlight">
\[i = (\ldots i_3 i_2 i_1)_b = i_1 + i_2 b + i_3 b^2 + \ldots\]</div>
<ol class="arabic simple" start="2">
<li><p>Compute the matrix-vector product</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{pmatrix} y_1 \\ y_2 \\ y_3 \\ \vdots \end{pmatrix} = C_j \begin{pmatrix} i_1 \\ i_2 \\ i_3 \\ \vdots \end{pmatrix}\end{split}\]</div>
<p>where all additions and multiplications are performed in base <span class="math notranslate nohighlight">\(b\)</span>.</p>
<ol class="arabic simple" start="3">
<li><p>Set the <span class="math notranslate nohighlight">\(j\)</span>th component of the <span class="math notranslate nohighlight">\(i\)</span>th points to</p></li>
</ol>
<div class="math notranslate nohighlight">
\[t^{(i)}_j = \frac{y_1}{b} + \frac{y_2}{b^2} + \frac{y_3}{b^3} + \ldots = (0.y_1y_2y_3\ldots)_b\]</div>
<p>The matrices <span class="math notranslate nohighlight">\(C_j\)</span>, <span class="math notranslate nohighlight">\(j=1, 2, \ldots, d\)</span> are known as <em>generating
matrices</em>, see <span id="id26">[<a class="reference internal" href="../../misc/bibliography.html#id56" title="Josef Dick and Friedrich Pillichshammer. Digital nets and sequences: discrepancy theory and quasi–Monte Carlo integration. Cambridge University Press, 2010.">DP10</a>]</span>.</p>
<p>We can encode the generating matrices as an integer matrix as follows. The
number of rows in the matrix determines the maximum dimension of the lattice
rule. The number of columns in the matrix determines the <code class="docutils literal notranslate"><span class="pre">log2</span></code> of the maximum
number of points. An integer on the <span class="math notranslate nohighlight">\(j\)</span>th row and <span class="math notranslate nohighlight">\(m\)</span>th column
encodes the <span class="math notranslate nohighlight">\(m\)</span>th column of the <span class="math notranslate nohighlight">\(j\)</span>th generating matrix
<span class="math notranslate nohighlight">\(C_j\)</span>. Since the <span class="math notranslate nohighlight">\(m\)</span>th column of <span class="math notranslate nohighlight">\(C_j\)</span> is a collection of
0’s and 1’s, it can be represented as an integer with <span class="math notranslate nohighlight">\(t\)</span> bits, where
<span class="math notranslate nohighlight">\(t\)</span> is the number of rows in the <span class="math notranslate nohighlight">\(j\)</span>th generating matrix
<span class="math notranslate nohighlight">\(C_j\)</span>. By default, the encoding assumes the integers are stored with
<em>least significant bit first</em> (LSB), so that the first integer on the <span class="math notranslate nohighlight">\(j\)</span>th row is 1. This LSB representation has two advantages.</p>
<ul class="simple">
<li><p>The integers can be reused if the number of bits <span class="math notranslate nohighlight">\(t\)</span> in the
representation changes.</p></li>
<li><p>It generally leads to smaller, human-readable numbers in the first few
entries.</p></li>
</ul>
<p>The Sobol sequence is a particularly famous example of a digital net
<span id="id27">[<a class="reference internal" href="../../misc/bibliography.html#id271" title="Il'ya Meerovich Sobol'. On the distribution of points in a cube and the approximate evaluation of integrals. Zhurnal Vychislitel'noi Matematiki i Matematicheskoi Fiziki, 7(4):784–802, 1967.">Sobol67</a>]</span>. A computer implementation of a Sobol sequence generator in
Fortran 77 was given by Bratley and Fox <span id="id28">[<a class="reference internal" href="../../misc/bibliography.html#id30" title="Paul Bratley and Bennett L Fox. Algorithm 659: Implementing Sobol's quasirandom sequence generator. ACM Transactions on Mathematical Software (TOMS), 14(1):88–100, 1988.">BF88</a>]</span> as Algorithm 659.
This implementation allowed points of up to 40 dimensions. It was extended by
Joe and Kuo to allow up to 1111 dimensions in <span id="id29">[<a class="reference internal" href="../../misc/bibliography.html#id163" title="Stephen Joe and Frances Y Kuo. Remark on algorithm 659: Implementing Sobol's quasirandom sequence generator. ACM Transactions on Mathematical Software (TOMS), 29(1):49–57, 2003.">JK03</a>]</span> and up to 21201
dimensions in <span id="id30">[<a class="reference internal" href="../../misc/bibliography.html#id164" title="Stephen Joe and Frances Y Kuo. Constructing Sobol sequences with better two-dimensional projections. SIAM Journal on Scientific Computing, 30(5):2635–2654, 2008.">JK08</a>]</span>. In the Dakota implementation of the algorithm
outlined above, we use the iterative construction from Antonov and Saleev
<span id="id31">[<a class="reference internal" href="../../misc/bibliography.html#id11" title="Ilya A Antonov and VM Saleev. An economic method of computing lpτ-sequences. USSR Computational Mathematics and Mathematical Physics, 19(1):252–256, 1979.">AS79</a>]</span>, that generates the points in Gray code ordering. Knowing
the current point with (Gray code) index <span class="math notranslate nohighlight">\(n\)</span>, the next point with index
<span class="math notranslate nohighlight">\(n + 1\)</span> is obtained by XOR’ing the current point with the <span class="math notranslate nohighlight">\(k\)</span>th
column of the <span class="math notranslate nohighlight">\(j\)</span>th generating matrix, i.e.,</p>
<div class="math notranslate nohighlight">
\[t^{(n+1)}_j = t^{(n)}_j \oplus C_{j, k}\]</div>
<p>where <span class="math notranslate nohighlight">\(k\)</span> is the rightmost zero-bit of <span class="math notranslate nohighlight">\(n\)</span> (the position of the bit that
will change from index <span class="math notranslate nohighlight">\(n\)</span> to <span class="math notranslate nohighlight">\(n+1\)</span> in Gray code).</p>
<p>The digital net points can be randomized by adding a <em>digital shift</em> vector. If
<span class="math notranslate nohighlight">\(\Delta\)</span> is a <span class="math notranslate nohighlight">\(d\)</span>-dimensional vector of standard normal random
variables, we construct the shifted lattice points as
<span class="math notranslate nohighlight">\(\boldsymbol{t}^{(i)} \otimes \Delta\)</span>, where <span class="math notranslate nohighlight">\(\otimes\)</span> is the
element-wise <span class="math notranslate nohighlight">\(b\)</span>-ary addition (or <code class="docutils literal notranslate"><span class="pre">XOR</span></code>) operator, see <span id="id32">[<a class="reference internal" href="../../misc/bibliography.html#id56" title="Josef Dick and Friedrich Pillichshammer. Digital nets and sequences: discrepancy theory and quasi–Monte Carlo integration. Cambridge University Press, 2010.">DP10</a>]</span>.
Note that the first untransformed point in the sequence will be
<span class="math notranslate nohighlight">\(\boldsymbol{t}^{(0)} = (0, 0, \ldots, 0)\)</span>.</p>
<p>Ideally, the digital net should preserve the structure of the points after
randomization. This can be achieved by <em>scrambling</em> the digital net. Scrambling
can also improve the rate of convergence of a method that uses these scrambled
points to compute the mean of the model response. Owen’s scrambling
<span id="id33">[<a class="reference internal" href="../../misc/bibliography.html#id228" title="Art B Owen. Scrambling Sobol'and Niederreiter–Xing points. Journal of complexity, 14(4):466–489, 1998.">Owe98</a>]</span> is the most well-known scrambling technique. A particular
variant is linear matrix scrambling, see <span id="id34">[<a class="reference internal" href="../../misc/bibliography.html#id196" title="Jiřı́ Matoušek. On the L2-discrepancy for anchored boxes. Journal of Complexity, 14(4):527–556, 1998.">Matouvsek98</a>]</span>, which is
implemented in Dakota. In linear matrix scrambling, we left-multiply each
generating matrix with a lower-triangular random scramble matrix with 1s on the
diagonal, i.e.,</p>
<div class="highlight-dakota notranslate"><div class="highlight"><pre><span></span>1 0 0 0 0
x 1 0 0 0
x x 1 0 0
x x x 1 0
x x x x 1
</pre></div>
</div>
<p>Finally, for the digital net points to be practically useful, we would like to
transform the digital net into a <em>digital sequence</em>, that allows us to generate
well-distributed points for an arbitrary number of points <span class="math notranslate nohighlight">\(N\)</span>. One way to
do this is to generate the points in Gray code ordering, as discussed above.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../theory.html" class="btn btn-neutral float-left" title="Dakota Theory" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="reliability.html" class="btn btn-neutral float-right" title="Reliability Methods" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <!--
  <div role="contentinfo">
    <p>&#169; Copyright 2024, Sandia National Laboratories.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
  
  --> 

</footer>
        </div>
      </div>
	  
	  <div style="background-color: #0f0f0f;color:#fafafa;padding:20px">
	    <div>
		  <h2><em>Exceptional service in the national interest</em></h2>
		</div>
		<p>© 2023 National Technology and Engineering Solutions of Sandia, LLC. | <a href="https://www.sandia.gov/contact_us/index.html">Questions &amp; Comments</a> | <a href="https://www.sandia.gov/general/privacy-security/index.html">Privacy &amp; Security</a></p>
		<p><a href="http://energy.gov" rel="noopener noreferrer" target="_blank"><img alt="U.S. Department of Energy" longdesc="https://energy.gov" src="https://www.sandia.gov/_common/images/doe_logo_white.png" style="height:37px; width:140px"></a> <a href="http://nnsa.energy.gov/" rel="noopener noreferrer" target="_blank"> <img alt="National Nuclear Security Administration" longdesc="http://nnsa.gov" src="https://www.sandia.gov/_common/images/nnsa_logo_white.png" style="height:37px; width:116px"></a></p>
		<p><a href="https://www.sandia.gov">Sandia National Laboratories</a> is a multimission laboratory managed and operated by National Technology and Engineering Solutions of Sandia, LLC., a wholly owned subsidiary of Honeywell International, Inc., for the U.S. Department of Energy’s National Nuclear Security Administration under contract DE-NA-0003525.</p>
	  </div>	  	  
	  
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>